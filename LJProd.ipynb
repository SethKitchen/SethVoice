{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04415985-e3eb-4310-9a90-c0f308006da2",
   "metadata": {},
   "source": [
    "# PIP INSTALLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5696370d-ab0d-44b0-af72-8859d641623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n",
      "Requirement already satisfied: soundfile in /opt/conda/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: librosa==0.9.1 in /opt/conda/lib/python3.10/site-packages (0.9.1)\n",
      "Requirement already satisfied: g2p_en in /opt/conda/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: audioread>=2.1.5 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (3.0.0)\n",
      "Requirement already satisfied: numba>=0.45.1 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (0.57.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (1.3.0)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (5.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (23.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (1.11.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (1.24.3)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (1.6.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: protobuf>=4.22.3 in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (4.23.4)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: distance>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from g2p_en) (0.1.3)\n",
      "Requirement already satisfied: nltk>=3.2.4 in /opt/conda/lib/python3.10/site-packages (from g2p_en) (3.8.1)\n",
      "Requirement already satisfied: inflect>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from g2p_en) (7.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.41.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from inflect>=0.3.1->g2p_en) (2.0.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from inflect>=0.3.1->g2p_en) (4.7.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk>=3.2.4->g2p_en) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.2.4->g2p_en) (2023.6.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.2.4->g2p_en) (8.1.6)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.45.1->librosa==0.9.1) (0.40.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.1) (2.29.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.1) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->librosa==0.9.1) (3.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9.1->inflect>=0.3.1->g2p_en) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.3.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9.1->inflect>=0.3.1->g2p_en) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (2023.5.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas tensorboardX sentencepiece soundfile librosa==0.9.1 g2p_en matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5bfac-4c6d-4fd1-8fc9-2f1b9d646c9e",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152d47d2-d7a5-45d3-ae5e-c42e447a4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Union\n",
    "\n",
    "from typing import Optional, List, Dict\n",
    "import zipfile\n",
    "import tempfile\n",
    "from dataclasses import dataclass\n",
    "from itertools import groupby\n",
    "\n",
    "import torchaudio\n",
    "from torch import Tensor\n",
    "from torch.hub import download_url_to_file\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio.datasets.utils import _extract_tar\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import shutil\n",
    "from tempfile import NamedTemporaryFile\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from fairseq.examples.speech_to_text.data_utils import save_df_to_tsv, create_zip, gen_config_yaml, gen_vocab, get_zip_manifest, load_tsv_to_dicts\n",
    "from fairseq.data.audio.audio_utils import convert_waveform, TTSSpectrogram, TTSMelScale, parse_path, read_from_stored_zip, is_npy_data\n",
    "from fairseq_cli import train\n",
    "from fairseq import options\n",
    "from fairseq.distributed import utils as distributed_utils\n",
    "from fairseq.examples.speech_synthesis import generate_waveform\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "SPLITS = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "out_path = '/workspace/production/LJ/'\n",
    "in_path = '/workspace/LJSpeech-1.1/wavs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19c1d6-8ff4-4592-a8c7-542c44790f61",
   "metadata": {},
   "source": [
    "# DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd588ff-6a18-491f-b6d3-741bc4889de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LJSPEECH(Dataset):\n",
    "    \"\"\"*LJSpeech-1.1* :cite:`ljspeech17` dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str or Path): Path to the directory where the dataset is found or downloaded.\n",
    "        url (str, optional): The URL to download the dataset from.\n",
    "            (default: ``\"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"``)\n",
    "        folder_in_archive (str, optional):\n",
    "            The top-level directory of the dataset. (default: ``\"wavs\"``)\n",
    "        download (bool, optional):\n",
    "            Whether to download the dataset if it is not found at root path. (default: ``False``).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        self._parse_filesystem()\n",
    "\n",
    "    def _parse_filesystem(self) -> None:\n",
    "        root = '/workspace/LJSpeech-1.1'\n",
    "\n",
    "        self._path = root + '/wavs'\n",
    "        self._metadata_path = root + \"/metadata.csv\"\n",
    "\n",
    "        with open(self._metadata_path, \"r\", newline=\"\") as metadata:\n",
    "            flist = csv.reader(metadata, delimiter=\"|\", quoting=csv.QUOTE_NONE)\n",
    "            self._flist = list(flist)\n",
    "\n",
    "    def __getitem__(self, n: int) -> Tuple[Tensor, int, str, str]:\n",
    "        \"\"\"Load the n-th sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            n (int): The index of the sample to be loaded\n",
    "\n",
    "        Returns:\n",
    "            Tuple of the following items;\n",
    "\n",
    "            Tensor:\n",
    "                Waveform\n",
    "            int:\n",
    "                Sample rate\n",
    "            str:\n",
    "                Transcript\n",
    "            str:\n",
    "                Normalized Transcript\n",
    "        \"\"\"\n",
    "        line = self._flist[n]\n",
    "        fileid, transcript, normalized_transcript = line\n",
    "        fileid_audio = self._path +'/'+(fileid + \".wav\")\n",
    "\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(fileid_audio)\n",
    "\n",
    "        return (\n",
    "            waveform,\n",
    "            sample_rate,\n",
    "            transcript,\n",
    "            normalized_transcript,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._flist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eafc05-98aa-454c-a47c-b485e66a3a6b",
   "metadata": {},
   "source": [
    "# Audio Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40fe74c-8408-4a6e-92d7-fde39319fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAudioManifest():\n",
    "    out_root = Path(out_path).absolute()\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Generate TSV manifest\n",
    "    print(\"Generating manifest...\")\n",
    "    # following FastSpeech's splits\n",
    "    dataset = LJSPEECH()\n",
    "    id_to_split = {}\n",
    "    for x in dataset._flist:\n",
    "        id_ = x[0]\n",
    "        speaker = id_.split(\"-\")[0]\n",
    "        id_to_split[id_] = {\n",
    "            \"LJ001\": \"test\", \"LJ002\": \"test\", \"LJ003\": \"dev\"\n",
    "        }.get(speaker, \"train\")\n",
    "    manifest_by_split = {split: defaultdict(list) for split in SPLITS}\n",
    "    progress = tqdm(enumerate(dataset), total=len(dataset))\n",
    "    for i, (waveform, _, utt, normalized_utt) in progress:\n",
    "        sample_id = dataset._flist[i][0]\n",
    "        split = id_to_split[sample_id]\n",
    "        manifest_by_split[split][\"id\"].append(sample_id)\n",
    "        audio_path = f\"{dataset._path}/{sample_id}.wav\"\n",
    "        manifest_by_split[split][\"audio\"].append(audio_path)\n",
    "        manifest_by_split[split][\"n_frames\"].append(len(waveform[0]))\n",
    "        manifest_by_split[split][\"tgt_text\"].append(normalized_utt)\n",
    "        manifest_by_split[split][\"speaker\"].append(\"ljspeech\")\n",
    "        manifest_by_split[split][\"src_text\"].append(utt)\n",
    "\n",
    "    manifest_root = Path(out_path).absolute()\n",
    "    manifest_root.mkdir(parents=True, exist_ok=True)\n",
    "    for split in SPLITS:\n",
    "        save_df_to_tsv(\n",
    "            pd.DataFrame.from_dict(manifest_by_split[split]),\n",
    "            manifest_root / f\"{split}.audio.tsv\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc4ae7c-74ef-478b-80df-7ffa3a3ecb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating manifest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13100/13100 [00:57<00:00, 228.39it/s]\n"
     ]
    }
   ],
   "source": [
    "processAudioManifest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05311e2-d38a-48cd-8fab-9192555de95c",
   "metadata": {},
   "source": [
    "# Feature Manifest Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d53b1a-0d7d-4303-84b8-c773792c7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_or_pad_to_target_length(\n",
    "        data_1d_or_2d: np.ndarray, target_length: int\n",
    ") -> np.ndarray:\n",
    "    assert len(data_1d_or_2d.shape) in {1, 2}\n",
    "    delta = data_1d_or_2d.shape[0] - target_length\n",
    "    if delta >= 0:  # trim if being longer\n",
    "        data_1d_or_2d = data_1d_or_2d[: target_length]\n",
    "    else:  # pad if being shorter\n",
    "        if len(data_1d_or_2d.shape) == 1:\n",
    "            data_1d_or_2d = np.concatenate(\n",
    "                [data_1d_or_2d, np.zeros(-delta)], axis=0\n",
    "            )\n",
    "        else:\n",
    "            data_1d_or_2d = np.concatenate(\n",
    "                [data_1d_or_2d, np.zeros((-delta, data_1d_or_2d.shape[1]))],\n",
    "                axis=0\n",
    "            )\n",
    "    return data_1d_or_2d\n",
    "\n",
    "\n",
    "def extract_logmel_spectrogram(\n",
    "        waveform: torch.Tensor, sample_rate: int,\n",
    "        output_path: Optional[Path] = None, win_length: int = 1024,\n",
    "        hop_length: int = 256, n_fft: int = 1024,\n",
    "        win_fn: callable = torch.hann_window, n_mels: int = 80,\n",
    "        f_min: float = 0., f_max: float = 8000, eps: float = 1e-5,\n",
    "        overwrite: bool = False, target_length: Optional[int] = None\n",
    "):\n",
    "    if output_path is not None and output_path.is_file() and not overwrite:\n",
    "        return\n",
    "\n",
    "    spectrogram_transform = TTSSpectrogram(\n",
    "        n_fft=n_fft, win_length=win_length, hop_length=hop_length,\n",
    "        window_fn=win_fn\n",
    "    )\n",
    "    mel_scale_transform = TTSMelScale(\n",
    "        n_mels=n_mels, sample_rate=sample_rate, f_min=f_min, f_max=f_max,\n",
    "        n_stft=n_fft // 2 + 1\n",
    "    )\n",
    "    spectrogram = spectrogram_transform(waveform)\n",
    "    mel_spec = mel_scale_transform(spectrogram)\n",
    "    logmel_spec = torch.clamp(mel_spec, min=eps).log()\n",
    "    assert len(logmel_spec.shape) == 3 and logmel_spec.shape[0] == 1\n",
    "    logmel_spec = logmel_spec.squeeze().t()  # D x T -> T x D\n",
    "    if target_length is not None:\n",
    "        logmel_spec = trim_or_pad_to_target_length(logmel_spec, target_length)\n",
    "\n",
    "    if output_path is not None:\n",
    "        np.save(output_path.as_posix(), logmel_spec)\n",
    "    else:\n",
    "        return logmel_spec\n",
    "\n",
    "\n",
    "def extract_pitch(\n",
    "        waveform: torch.Tensor, sample_rate: int,\n",
    "        output_path: Optional[Path] = None, hop_length: int = 256,\n",
    "        log_scale: bool = True, phoneme_durations: Optional[List[int]] = None\n",
    "):\n",
    "    if output_path is not None and output_path.is_file():\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        import pyworld\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install PyWORLD: pip install pyworld\")\n",
    "\n",
    "    _waveform = waveform.squeeze(0).double().numpy()\n",
    "    pitch, t = pyworld.dio(\n",
    "        _waveform, sample_rate, frame_period=hop_length / sample_rate * 1000\n",
    "    )\n",
    "    pitch = pyworld.stonemask(_waveform, pitch, t, sample_rate)\n",
    "\n",
    "    if phoneme_durations is not None:\n",
    "        pitch = trim_or_pad_to_target_length(pitch, sum(phoneme_durations))\n",
    "        try:\n",
    "            from scipy.interpolate import interp1d\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install SciPy: pip install scipy\")\n",
    "        nonzero_ids = np.where(pitch != 0)[0]\n",
    "        if len(nonzero_ids) == 0:\n",
    "            print((f\"{output_path} has all empty values in the pitch contour\"))\n",
    "            return\n",
    "        elif len(nonzero_ids) == 1:\n",
    "            print((f\"{output_path} has only one non-zero values in the pitch contour\"))\n",
    "            return\n",
    "        else:\n",
    "            interp_fn = interp1d(\n",
    "                nonzero_ids,\n",
    "                pitch[nonzero_ids],\n",
    "                fill_value=(pitch[nonzero_ids[0]], pitch[nonzero_ids[-1]]),\n",
    "                bounds_error=False,\n",
    "            )\n",
    "            pitch = interp_fn(np.arange(0, len(pitch)))\n",
    "        d_cumsum = np.cumsum(np.concatenate([np.array([0]), phoneme_durations]))\n",
    "        pitch = np.array(\n",
    "            [\n",
    "                np.mean(pitch[d_cumsum[i-1]: d_cumsum[i]])\n",
    "                for i in range(1, len(d_cumsum))\n",
    "            ]\n",
    "        )\n",
    "        assert len(pitch) == len(phoneme_durations)\n",
    "\n",
    "    if log_scale:\n",
    "        pitch = np.log(pitch + 1)\n",
    "\n",
    "    if output_path is not None:\n",
    "        np.save(output_path.as_posix(), pitch)\n",
    "    else:\n",
    "        return pitch\n",
    "\n",
    "\n",
    "def extract_energy(\n",
    "        waveform: torch.Tensor, output_path: Optional[Path] = None,\n",
    "        hop_length: int = 256, n_fft: int = 1024, log_scale: bool = True,\n",
    "        phoneme_durations: Optional[List[int]] = None\n",
    "):\n",
    "    if output_path is not None and output_path.is_file():\n",
    "        return\n",
    "\n",
    "    assert len(waveform.shape) == 2 and waveform.shape[0] == 1\n",
    "    waveform = waveform.view(1, 1, waveform.shape[1])\n",
    "    waveform = F.pad(\n",
    "        waveform.unsqueeze(1), [n_fft // 2, n_fft // 2, 0, 0],\n",
    "        mode=\"reflect\"\n",
    "    )\n",
    "    waveform = waveform.squeeze(1)\n",
    "\n",
    "    fourier_basis = np.fft.fft(np.eye(n_fft))\n",
    "    cutoff = int((n_fft / 2 + 1))\n",
    "    fourier_basis = np.vstack(\n",
    "        [np.real(fourier_basis[:cutoff, :]),\n",
    "         np.imag(fourier_basis[:cutoff, :])]\n",
    "    )\n",
    "\n",
    "    forward_basis = torch.FloatTensor(fourier_basis[:, None, :])\n",
    "    forward_transform = F.conv1d(\n",
    "        waveform, forward_basis, stride=hop_length, padding=0\n",
    "    )\n",
    "\n",
    "    real_part = forward_transform[:, :cutoff, :]\n",
    "    imag_part = forward_transform[:, cutoff:, :]\n",
    "    magnitude = torch.sqrt(real_part ** 2 + imag_part ** 2)\n",
    "    energy = torch.norm(magnitude, dim=1).squeeze(0).numpy()\n",
    "\n",
    "    if phoneme_durations is not None:\n",
    "        energy = trim_or_pad_to_target_length(energy, sum(phoneme_durations))\n",
    "        d_cumsum = np.cumsum(np.concatenate([np.array([0]), phoneme_durations]))\n",
    "        energy = np.array(\n",
    "            [\n",
    "                np.mean(energy[d_cumsum[i - 1]: d_cumsum[i]])\n",
    "                for i in range(1, len(d_cumsum))\n",
    "            ]\n",
    "        )\n",
    "        assert len(energy) == len(phoneme_durations)\n",
    "\n",
    "    if log_scale:\n",
    "        energy = np.log(energy + 1)\n",
    "\n",
    "    if output_path is not None:\n",
    "        np.save(output_path.as_posix(), energy)\n",
    "    else:\n",
    "        return energy\n",
    "\n",
    "\n",
    "def get_global_cmvn(feature_root: Path, output_path: Optional[Path] = None):\n",
    "    mean_x, mean_x2, n_frames = None, None, 0\n",
    "    feature_paths = feature_root.glob(\"*.npy\")\n",
    "    for p in tqdm(feature_paths):\n",
    "        with open(p, 'rb') as f:\n",
    "            frames = np.load(f).squeeze()\n",
    "\n",
    "        n_frames += frames.shape[0]\n",
    "\n",
    "        cur_mean_x = frames.sum(axis=0)\n",
    "        if mean_x is None:\n",
    "            mean_x = cur_mean_x\n",
    "        else:\n",
    "            mean_x += cur_mean_x\n",
    "\n",
    "        cur_mean_x2 = (frames ** 2).sum(axis=0)\n",
    "        if mean_x2 is None:\n",
    "            mean_x2 = cur_mean_x2\n",
    "        else:\n",
    "            mean_x2 += cur_mean_x2\n",
    "\n",
    "    mean_x /= n_frames\n",
    "    mean_x2 /= n_frames\n",
    "    var_x = mean_x2 - mean_x ** 2\n",
    "    std_x = np.sqrt(np.maximum(var_x, 1e-10))\n",
    "\n",
    "    if output_path is not None:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            np.savez(f, mean=mean_x, std=std_x)\n",
    "    else:\n",
    "        return {\"mean\": mean_x, \"std\": std_x}\n",
    "\n",
    "\n",
    "def ipa_phonemize(text, lang=\"en-us\", use_g2p=False):\n",
    "    if use_g2p:\n",
    "        assert lang == \"en-us\", \"g2pE phonemizer only works for en-us\"\n",
    "        try:\n",
    "            from g2p_en import G2p\n",
    "            g2p = G2p()\n",
    "            return \" \".join(\"|\" if p == \" \" else p for p in g2p(text))\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install phonemizer: pip install g2p_en\"\n",
    "            )\n",
    "    else:\n",
    "        try:\n",
    "            from phonemizer import phonemize\n",
    "            from phonemizer.separator import Separator\n",
    "            return phonemize(\n",
    "                text, backend='espeak', language=lang,\n",
    "                separator=Separator(word=\"| \", phone=\" \")\n",
    "            )\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install phonemizer: pip install phonemizer\"\n",
    "            )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ForceAlignmentInfo(object):\n",
    "    tokens: List[str]\n",
    "    frame_durations: List[int]\n",
    "    start_sec: Optional[float]\n",
    "    end_sec: Optional[float]\n",
    "\n",
    "\n",
    "def get_mfa_alignment_by_sample_id(\n",
    "        textgrid_zip_path: str, sample_id: str, sample_rate: int,\n",
    "        hop_length: int, silence_phones: List[str] = (\"sil\", \"sp\", \"spn\")\n",
    ") -> ForceAlignmentInfo:\n",
    "    try:\n",
    "        import tgt\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install TextGridTools: pip install tgt\")\n",
    "\n",
    "    filename = f\"{sample_id}.TextGrid\"\n",
    "    out_root = Path(tempfile.gettempdir())\n",
    "    tgt_path = out_root / filename\n",
    "    with zipfile.ZipFile(textgrid_zip_path) as f_zip:\n",
    "        f_zip.extract(filename, path=out_root)\n",
    "    textgrid = tgt.io.read_textgrid(tgt_path.as_posix())\n",
    "    os.remove(tgt_path)\n",
    "\n",
    "    phones, frame_durations = [], []\n",
    "    start_sec, end_sec, end_idx = 0, 0, 0\n",
    "    for t in textgrid.get_tier_by_name(\"phones\")._objects:\n",
    "        s, e, p = t.start_time, t.end_time, t.text\n",
    "        # Trim leading silences\n",
    "        if len(phones) == 0:\n",
    "            if p in silence_phones:\n",
    "                continue\n",
    "            else:\n",
    "                start_sec = s\n",
    "        phones.append(p)\n",
    "        if p not in silence_phones:\n",
    "            end_sec = e\n",
    "            end_idx = len(phones)\n",
    "        r = sample_rate / hop_length\n",
    "        frame_durations.append(int(np.round(e * r) - np.round(s * r)))\n",
    "    # Trim tailing silences\n",
    "    phones = phones[:end_idx]\n",
    "    frame_durations = frame_durations[:end_idx]\n",
    "\n",
    "    return ForceAlignmentInfo(\n",
    "        tokens=phones, frame_durations=frame_durations, start_sec=start_sec,\n",
    "        end_sec=end_sec\n",
    "    )\n",
    "\n",
    "\n",
    "def get_mfa_alignment(\n",
    "        textgrid_zip_path: str, sample_ids: List[str], sample_rate: int,\n",
    "        hop_length: int\n",
    ") -> Dict[str, ForceAlignmentInfo]:\n",
    "    return {\n",
    "        i: get_mfa_alignment_by_sample_id(\n",
    "            textgrid_zip_path, i, sample_rate, hop_length\n",
    "        ) for i in tqdm(sample_ids)\n",
    "    }\n",
    "\n",
    "\n",
    "def get_unit_alignment(\n",
    "        id_to_unit_tsv_path: str, sample_ids: List[str]\n",
    ") -> Dict[str, ForceAlignmentInfo]:\n",
    "    id_to_units = {\n",
    "        e[\"id\"]: e[\"units\"] for e in load_tsv_to_dicts(id_to_unit_tsv_path)\n",
    "    }\n",
    "    id_to_units = {i: id_to_units[i].split() for i in sample_ids}\n",
    "    id_to_units_collapsed = {\n",
    "        i: [uu for uu, _ in groupby(u)] for i, u in id_to_units.items()\n",
    "    }\n",
    "    id_to_durations = {\n",
    "        i: [len(list(g)) for _, g in groupby(u)] for i, u in id_to_units.items()\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        i: ForceAlignmentInfo(\n",
    "            tokens=id_to_units_collapsed[i], frame_durations=id_to_durations[i],\n",
    "            start_sec=None, end_sec=None\n",
    "        )\n",
    "        for i in sample_ids\n",
    "    }\n",
    "\n",
    "\n",
    "def get_feature_value_min_max(feature_paths: List[str]):\n",
    "    v_min, v_max = 1e-8, -1e-8\n",
    "    for p in tqdm(feature_paths):\n",
    "        _path, slice_ptr = parse_path(p)\n",
    "        assert len(slice_ptr) == 2\n",
    "        byte_data = read_from_stored_zip(_path, slice_ptr[0], slice_ptr[1])\n",
    "        assert is_npy_data(byte_data)\n",
    "        path_or_fp = io.BytesIO(byte_data)\n",
    "        features = np.load(path_or_fp).squeeze()\n",
    "        v_min = min(v_min, features.min().item())\n",
    "        v_max = max(v_max, features.max().item())\n",
    "    return v_min, v_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fb175-ce53-45fa-b1a3-098d9a327e93",
   "metadata": {},
   "source": [
    "# LOG-MEL SPECTROGRAM FEATURE MANIFEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3be6086-4199-4b3d-8840-dd98a021e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFeatureManifest(args):\n",
    "    assert \"train\" in args.splits\n",
    "    out_root = Path(args.output_root).absolute()\n",
    "    out_root.mkdir(exist_ok=True)\n",
    "\n",
    "    print(\"Fetching data...\")\n",
    "    audio_manifest_root = Path(args.audio_manifest_root).absolute()\n",
    "    samples = []\n",
    "    for s in args.splits:\n",
    "        for e in load_tsv_to_dicts(audio_manifest_root / f\"{s}.audio.tsv\"):\n",
    "            e[\"split\"] = s\n",
    "            samples.append(e)\n",
    "    sample_ids = [s[\"id\"] for s in samples]\n",
    "\n",
    "    # Get alignment info\n",
    "    id_to_alignment = None\n",
    "    if args.textgrid_zip is not None:\n",
    "        assert args.id_to_units_tsv is None\n",
    "        id_to_alignment = get_mfa_alignment(\n",
    "            args.textgrid_zip, sample_ids, args.sample_rate, args.hop_length\n",
    "        )\n",
    "    elif args.id_to_units_tsv is not None:\n",
    "        # assume identical hop length on the unit sequence\n",
    "        id_to_alignment = get_unit_alignment(args.id_to_units_tsv, sample_ids)\n",
    "\n",
    "    # Extract features and pack features into ZIP\n",
    "    feature_name = \"logmelspec80\"\n",
    "    zip_path = out_root / f\"{feature_name}.zip\"\n",
    "    pitch_zip_path = out_root / \"pitch.zip\"\n",
    "    energy_zip_path = out_root / \"energy.zip\"\n",
    "    gcmvn_npz_path = out_root / \"gcmvn_stats.npz\"\n",
    "    if zip_path.exists() and gcmvn_npz_path.exists():\n",
    "        print(f\"{zip_path} and {gcmvn_npz_path} exist.\")\n",
    "    else:\n",
    "        feature_root = out_root / feature_name\n",
    "        feature_root.mkdir(exist_ok=True)\n",
    "        pitch_root = out_root / \"pitch\"\n",
    "        energy_root = out_root / \"energy\"\n",
    "        if args.add_fastspeech_targets:\n",
    "            pitch_root.mkdir(exist_ok=True)\n",
    "            energy_root.mkdir(exist_ok=True)\n",
    "        print(\"Extracting Mel spectrogram features...\")\n",
    "        for sample in tqdm(samples):\n",
    "            waveform, sample_rate = torchaudio.load(sample[\"audio\"])\n",
    "            waveform, sample_rate = convert_waveform(\n",
    "                waveform, sample_rate, normalize_volume=args.normalize_volume,\n",
    "                to_sample_rate=args.sample_rate\n",
    "            )\n",
    "            sample_id = sample[\"id\"]\n",
    "            target_length = None\n",
    "            if id_to_alignment is not None:\n",
    "                a = id_to_alignment[sample_id]\n",
    "                target_length = sum(a.frame_durations)\n",
    "                if a.start_sec is not None and a.end_sec is not None:\n",
    "                    start_frame = int(a.start_sec * sample_rate)\n",
    "                    end_frame = int(a.end_sec * sample_rate)\n",
    "                    waveform = waveform[:, start_frame: end_frame]\n",
    "            extract_logmel_spectrogram(\n",
    "                waveform, sample_rate, feature_root / f\"{sample_id}.npy\",\n",
    "                win_length=args.win_length, hop_length=args.hop_length,\n",
    "                n_fft=args.n_fft, n_mels=args.n_mels, f_min=args.f_min,\n",
    "                f_max=args.f_max, target_length=target_length\n",
    "            )\n",
    "            if args.add_fastspeech_targets:\n",
    "                assert id_to_alignment is not None\n",
    "                extract_pitch(\n",
    "                    waveform, sample_rate, pitch_root / f\"{sample_id}.npy\",\n",
    "                    hop_length=args.hop_length, log_scale=True,\n",
    "                    phoneme_durations=id_to_alignment[sample_id].frame_durations\n",
    "                )\n",
    "                extract_energy(\n",
    "                    waveform, energy_root / f\"{sample_id}.npy\",\n",
    "                    hop_length=args.hop_length, n_fft=args.n_fft,\n",
    "                    log_scale=True,\n",
    "                    phoneme_durations=id_to_alignment[sample_id].frame_durations\n",
    "                )\n",
    "        print(\"ZIPing features...\")\n",
    "        create_zip(feature_root, zip_path)\n",
    "        get_global_cmvn(feature_root, gcmvn_npz_path)\n",
    "        shutil.rmtree(feature_root)\n",
    "        if args.add_fastspeech_targets:\n",
    "            create_zip(pitch_root, pitch_zip_path)\n",
    "            shutil.rmtree(pitch_root)\n",
    "            create_zip(energy_root, energy_zip_path)\n",
    "            shutil.rmtree(energy_root)\n",
    "\n",
    "    print(\"Fetching ZIP manifest...\")\n",
    "    audio_paths, audio_lengths = get_zip_manifest(zip_path)\n",
    "    pitch_paths, pitch_lengths, energy_paths, energy_lengths = [None] * 4\n",
    "    if args.add_fastspeech_targets:\n",
    "        pitch_paths, pitch_lengths = get_zip_manifest(pitch_zip_path)\n",
    "        energy_paths, energy_lengths = get_zip_manifest(energy_zip_path)\n",
    "    # Generate TSV manifest\n",
    "    print(\"Generating manifest...\")\n",
    "    id_to_cer = None\n",
    "    if args.cer_threshold is not None:\n",
    "        assert Path(args.cer_tsv_path).is_file()\n",
    "        id_to_cer = {\n",
    "            x[\"id\"]: x[\"uer\"] for x in load_tsv_to_dicts(args.cer_tsv_path)\n",
    "        }\n",
    "    manifest_by_split = {split: defaultdict(list) for split in args.splits}\n",
    "    for sample in tqdm(samples):\n",
    "        sample_id, split = sample[\"id\"], sample[\"split\"]\n",
    "\n",
    "        if args.snr_threshold is not None and \"snr\" in sample \\\n",
    "                and sample[\"snr\"] < args.snr_threshold:\n",
    "            continue\n",
    "        if args.cer_threshold is not None \\\n",
    "                and id_to_cer[sample_id] > args.cer_threhold:\n",
    "            continue\n",
    "\n",
    "        normalized_utt = sample[\"tgt_text\"]\n",
    "        if id_to_alignment is not None:\n",
    "            normalized_utt = \" \".join(id_to_alignment[sample_id].tokens)\n",
    "        elif args.ipa_vocab:\n",
    "            normalized_utt = ipa_phonemize(\n",
    "                normalized_utt, lang=args.lang, use_g2p=args.use_g2p\n",
    "            )\n",
    "        manifest_by_split[split][\"id\"].append(sample_id)\n",
    "        manifest_by_split[split][\"audio\"].append(audio_paths[sample_id])\n",
    "        manifest_by_split[split][\"n_frames\"].append(audio_lengths[sample_id])\n",
    "        manifest_by_split[split][\"tgt_text\"].append(normalized_utt)\n",
    "        manifest_by_split[split][\"speaker\"].append(sample[\"speaker\"])\n",
    "        manifest_by_split[split][\"src_text\"].append(sample[\"src_text\"])\n",
    "        if args.add_fastspeech_targets:\n",
    "            assert id_to_alignment is not None\n",
    "            duration = \" \".join(\n",
    "                str(d) for d in id_to_alignment[sample_id].frame_durations\n",
    "            )\n",
    "            manifest_by_split[split][\"duration\"].append(duration)\n",
    "            manifest_by_split[split][\"pitch\"].append(pitch_paths[sample_id])\n",
    "            manifest_by_split[split][\"energy\"].append(energy_paths[sample_id])\n",
    "    for split in args.splits:\n",
    "        save_df_to_tsv(\n",
    "            pd.DataFrame.from_dict(manifest_by_split[split]),\n",
    "            out_root / f\"{split}.tsv\"\n",
    "        )\n",
    "    # Generate vocab\n",
    "    vocab_name, spm_filename = None, None\n",
    "    if id_to_alignment is not None or args.ipa_vocab:\n",
    "        vocab = Counter()\n",
    "        for t in manifest_by_split[\"train\"][\"tgt_text\"]:\n",
    "            vocab.update(t.split(\" \"))\n",
    "        vocab_name = \"vocab.txt\"\n",
    "        with open(out_root / vocab_name, \"w\") as f:\n",
    "            for s, c in vocab.most_common():\n",
    "                f.write(f\"{s} {c}\\n\")\n",
    "    else:\n",
    "        spm_filename_prefix = \"spm_char\"\n",
    "        spm_filename = f\"{spm_filename_prefix}.model\"\n",
    "        with NamedTemporaryFile(mode=\"w\") as f:\n",
    "            for t in manifest_by_split[\"train\"][\"tgt_text\"]:\n",
    "                f.write(t + \"\\n\")\n",
    "            f.flush()  # needed to ensure gen_vocab sees dumped text\n",
    "            gen_vocab(Path(f.name), out_root / spm_filename_prefix, \"char\")\n",
    "    # Generate speaker list\n",
    "    speakers = sorted({sample[\"speaker\"] for sample in samples})\n",
    "    speakers_path = out_root / \"speakers.txt\"\n",
    "    with open(speakers_path, \"w\") as f:\n",
    "        for speaker in speakers:\n",
    "            f.write(f\"{speaker}\\n\")\n",
    "    # Generate config YAML\n",
    "    win_len_t = args.win_length / args.sample_rate\n",
    "    hop_len_t = args.hop_length / args.sample_rate\n",
    "    extra = {\n",
    "        \"sample_rate\": args.sample_rate,\n",
    "        \"features\": {\n",
    "            \"type\": \"spectrogram+melscale+log\",\n",
    "            \"eps\": 1e-5, \"n_mels\": args.n_mels, \"n_fft\": args.n_fft,\n",
    "            \"window_fn\": \"hann\", \"win_length\": args.win_length,\n",
    "            \"hop_length\": args.hop_length, \"sample_rate\": args.sample_rate,\n",
    "            \"win_len_t\": win_len_t, \"hop_len_t\": hop_len_t,\n",
    "            \"f_min\": args.f_min, \"f_max\": args.f_max,\n",
    "            \"n_stft\": args.n_fft // 2 + 1\n",
    "        }\n",
    "    }\n",
    "    if len(speakers) > 1:\n",
    "        extra[\"speaker_set_filename\"] = \"speakers.txt\"\n",
    "    if args.add_fastspeech_targets:\n",
    "        pitch_min, pitch_max = get_feature_value_min_max(\n",
    "            [(out_root / n).as_posix() for n in pitch_paths.values()]\n",
    "        )\n",
    "        energy_min, energy_max = get_feature_value_min_max(\n",
    "            [(out_root / n).as_posix() for n in energy_paths.values()]\n",
    "        )\n",
    "        extra[\"features\"][\"pitch_min\"] = pitch_min\n",
    "        extra[\"features\"][\"pitch_max\"] = pitch_max\n",
    "        extra[\"features\"][\"energy_min\"] = energy_min\n",
    "        extra[\"features\"][\"energy_max\"] = energy_max\n",
    "    gen_config_yaml(\n",
    "        out_root, spm_filename=spm_filename, vocab_name=vocab_name,\n",
    "        audio_root=out_root.as_posix(), input_channels=None,\n",
    "        input_feat_per_channel=None, specaugment_policy=None,\n",
    "        cmvn_type=\"global\", gcmvn_path=gcmvn_npz_path, extra=extra\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaac8d79-2b16-4c47-aad7-68caab5c8e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(audio_manifest_root='/workspace/process/LJ/', output_root='/workspace/process/LJ/', splits=['train', 'dev', 'test'], ipa_vocab=True, use_g2p=True, lang='en-us', win_length=1024, hop_length=256, n_fft=1024, n_mels=80, f_min=20, f_max=8000, sample_rate=22050, normalize_volume=False, textgrid_zip=None, id_to_units_tsv=None, add_fastspeech_targets=False, snr_threshold=None, cer_threshold=None, cer_tsv_path='')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--audio-manifest-root\", \"-m\", required=True, type=str)\n",
    "parser.add_argument(\"--output-root\", \"-o\", required=True, type=str)\n",
    "parser.add_argument(\"--splits\", \"-s\", type=str, nargs=\"+\",\n",
    "                    default=[\"train\", \"dev\", \"test\"])\n",
    "parser.add_argument(\"--ipa-vocab\", action=\"store_true\")\n",
    "parser.add_argument(\"--use-g2p\", action=\"store_true\")\n",
    "parser.add_argument(\"--lang\", type=str, default=\"en-us\")\n",
    "parser.add_argument(\"--win-length\", type=int, default=1024)\n",
    "parser.add_argument(\"--hop-length\", type=int, default=256)\n",
    "parser.add_argument(\"--n-fft\", type=int, default=1024)\n",
    "parser.add_argument(\"--n-mels\", type=int, default=80)\n",
    "parser.add_argument(\"--f-min\", type=int, default=20)\n",
    "parser.add_argument(\"--f-max\", type=int, default=8000)\n",
    "parser.add_argument(\"--sample-rate\", type=int, default=22050)\n",
    "parser.add_argument(\"--normalize-volume\", \"-n\", action=\"store_true\")\n",
    "parser.add_argument(\"--textgrid-zip\", type=str, default=None)\n",
    "parser.add_argument(\"--id-to-units-tsv\", type=str, default=None)\n",
    "parser.add_argument(\"--add-fastspeech-targets\", action=\"store_true\")\n",
    "parser.add_argument(\"--snr-threshold\", type=float, default=None)\n",
    "parser.add_argument(\"--cer-threshold\", type=float, default=None)\n",
    "parser.add_argument(\"--cer-tsv-path\", type=str, default=\"\")\n",
    "args = parser.parse_args(['--audio-manifest-root', out_path, '--output-root', out_path, '--ipa-vocab', '--use-g2p']) #, '--add-fastspeech-targets'])\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6a17983-0d23-4d8f-8793-da7cfcfbf3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...\n",
      "/workspace/process/LJ/logmelspec80.zip and /workspace/process/LJ/gcmvn_stats.npz exist.\n",
      "Fetching ZIP manifest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13100/13100 [00:23<00:00, 552.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating manifest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13100/13100 [2:36:59<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "processFeatureManifest(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7644c9d8-0076-416e-9c0d-110936f75815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac31c81-e930-42cf-9d00-19f927235a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA TITAN RTX'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c26bd-8ad5-4abf-960b-b22472bc6310",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc1bfa3-6a6f-4a5e-bf09-8323afdb5b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='tacotron2', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='text_to_speech', num_workers=3, skip_invalid_size_inputs_valid_test=False, max_tokens=30000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='dev', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=30000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='tts_transformer', max_epoch=0, max_update=200000, stop_time_hours=0, clip_norm=5.0, sentence_avg=False, update_freq=[5], lr=[0.002], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/workspace/process/LJ/', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='mcd_loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, activation_fn='relu', data='/workspace/process/LJ/', config_yaml='config.yaml', max_source_positions=1024, max_target_positions=1200, n_frames_per_step=4, eos_prob_threshold=0.5, eval_inference=True, eval_tb_nsample=8, vocoder='griffin_lim', spec_bwd_max_iter=8, bce_pos_weight=5.0, use_guided_attention_loss=False, guided_attention_loss_sigma=0.4, ctc_weight=0.0, adam_betas=(0.9, 0.999), adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, encoder_normalize_before=True, decoder_normalize_before=True, no_seed_provided=False, output_frame_dim=80, speaker_embed_dim=64, encoder_dropout=0.5, encoder_conv_layers=3, encoder_conv_kernel_size=5, encoder_transformer_layers=6, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_attention_heads=4, prenet_dropout=0.5, prenet_layers=2, prenet_dim=256, postnet_dropout=0.5, postnet_layers=5, postnet_conv_dim=512, postnet_conv_kernel_size=5, decoder_transformer_layers=6, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_attention_heads=4)\n"
     ]
    }
   ],
   "source": [
    "trainParser = options.get_training_parser()\n",
    "args = options.parse_args_and_arch(trainParser, [out_path, '--save-dir', out_path, \n",
    "                          '--config-yaml', 'config.yaml', '--train-subset', 'train', '--valid-subset', 'dev', \n",
    "                          '--num-workers', '3', '--max-tokens', '30000', '--max-update', '200000', \n",
    "                          '--task', 'text_to_speech', '--criterion', 'tacotron2', '--arch', 'tts_transformer',\n",
    "                          '--clip-norm', '5.0', '--n-frames-per-step', '4', '--bce-pos-weight', '5.0',\n",
    "                          '--dropout', '0.1', '--attention-dropout', '0.1', '--activation-dropout', '0.1',\n",
    "                          '--encoder-normalize-before', '--decoder-normalize-before', \n",
    "                          '--optimizer', 'adam', '--lr', '2e-3', '--lr-scheduler', 'inverse_sqrt', '--warmup-updates', '4000',\n",
    "                          '--seed', '1', '--update-freq', '5', '--eval-inference', '--best-checkpoint-metric', 'mcd_loss'])\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cda266c-56f5-4e9a-bcc0-aef44b3991c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 18:21:44 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 3, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 30000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'dev', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 30000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 200000, 'stop_time_hours': 0.0, 'clip_norm': 5.0, 'sentence_avg': False, 'update_freq': [5], 'lr': [0.002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/workspace/process/LJ/', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'mcd_loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='tacotron2', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='text_to_speech', num_workers=3, skip_invalid_size_inputs_valid_test=False, max_tokens=30000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='dev', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=30000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='tts_transformer', max_epoch=0, max_update=200000, stop_time_hours=0, clip_norm=5.0, sentence_avg=False, update_freq=[5], lr=[0.002], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/workspace/process/LJ/', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='mcd_loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, activation_fn='relu', data='/workspace/process/LJ/', config_yaml='config.yaml', max_source_positions=1024, max_target_positions=1200, n_frames_per_step=4, eos_prob_threshold=0.5, eval_inference=True, eval_tb_nsample=8, vocoder='griffin_lim', spec_bwd_max_iter=8, bce_pos_weight=5.0, use_guided_attention_loss=False, guided_attention_loss_sigma=0.4, ctc_weight=0.0, adam_betas=(0.9, 0.999), adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, encoder_normalize_before=True, decoder_normalize_before=True, no_seed_provided=False, output_frame_dim=80, speaker_embed_dim=64, encoder_dropout=0.5, encoder_conv_layers=3, encoder_conv_kernel_size=5, encoder_transformer_layers=6, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_attention_heads=4, prenet_dropout=0.5, prenet_layers=2, prenet_dim=256, postnet_dropout=0.5, postnet_layers=5, postnet_conv_dim=512, postnet_conv_kernel_size=5, decoder_transformer_layers=6, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_attention_heads=4, _name='tts_transformer'), 'task': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='tacotron2', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='text_to_speech', num_workers=3, skip_invalid_size_inputs_valid_test=False, max_tokens=30000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='dev', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=30000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='tts_transformer', max_epoch=0, max_update=200000, stop_time_hours=0, clip_norm=5.0, sentence_avg=False, update_freq=[5], lr=[0.002], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/workspace/process/LJ/', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='mcd_loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, activation_fn='relu', data='/workspace/process/LJ/', config_yaml='config.yaml', max_source_positions=1024, max_target_positions=1200, n_frames_per_step=4, eos_prob_threshold=0.5, eval_inference=True, eval_tb_nsample=8, vocoder='griffin_lim', spec_bwd_max_iter=8, bce_pos_weight=5.0, use_guided_attention_loss=False, guided_attention_loss_sigma=0.4, ctc_weight=0.0, adam_betas=(0.9, 0.999), adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, encoder_normalize_before=True, decoder_normalize_before=True, no_seed_provided=False, output_frame_dim=80, speaker_embed_dim=64, encoder_dropout=0.5, encoder_conv_layers=3, encoder_conv_kernel_size=5, encoder_transformer_layers=6, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_attention_heads=4, prenet_dropout=0.5, prenet_layers=2, prenet_dim=256, postnet_dropout=0.5, postnet_layers=5, postnet_conv_dim=512, postnet_conv_kernel_size=5, decoder_transformer_layers=6, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_attention_heads=4, _name='text_to_speech'), 'criterion': {'_name': 'tacotron2', 'bce_pos_weight': 5.0, 'use_guided_attention_loss': False, 'guided_attention_loss_sigma': 0.4, 'ctc_weight': 0.0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-24 18:21:44 | INFO | fairseq.tasks.speech_to_text | dictionary size (vocab.txt): 81\n",
      "2023-07-24 18:21:45 | INFO | fairseq_cli.train | TTSTransformerModel(\n",
      "  (encoder): TTSTransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(81, 512, padding_idx=1)\n",
      "    (prenet): ModuleList(\n",
      "      (0-2): 3 x Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (prenet_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TTSTransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (prenet): Sequential(\n",
      "      (0): Prenet(\n",
      "        (layers): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Linear(in_features=320, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "    )\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (feat_proj): Linear(in_features=512, out_features=320, bias=True)\n",
      "    (eos_proj): Linear(in_features=512, out_features=1, bias=True)\n",
      "    (postnet): Postnet(\n",
      "      (convolutions): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv1d(320, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Tanh()\n",
      "          (3): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (1-3): 3 x Sequential(\n",
      "          (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Tanh()\n",
      "          (3): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): Conv1d(512, 320, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "          (1): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2023-07-24 18:21:45 | INFO | fairseq_cli.train | task: TextToSpeechTask\n",
      "2023-07-24 18:21:45 | INFO | fairseq_cli.train | model: TTSTransformerModel\n",
      "2023-07-24 18:21:45 | INFO | fairseq_cli.train | criterion: Tacotron2Criterion\n",
      "2023-07-24 18:21:45 | INFO | fairseq_cli.train | num. shared model params: 73,305,347 (num. trained: 73,305,347)\n",
      "2023-07-24 18:21:45 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-07-24 18:21:45 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2023-07-24 18:21:45 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': None}\n",
      "2023-07-24 18:21:45 | INFO | fairseq.data.audio.speech_to_text_dataset | 'dev' has 0.00% OOV\n",
      "2023-07-24 18:21:45 | INFO | fairseq.data.audio.speech_to_text_dataset | TextToSpeechDataset(split=\"dev\", n_samples=348, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    GlobalCMVN(stats_npz_path=\"/workspace/process/LJ/gcmvn_stats.npz\")\n",
      "), n_frames_per_step=4\n",
      "2023-07-24 18:21:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-07-24 18:21:45 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 24.000 GB ; name = NVIDIA TITAN RTX                        \n",
      "2023-07-24 18:21:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-07-24 18:21:46 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-07-24 18:21:46 | INFO | fairseq_cli.train | max tokens per device = 30000 and max sentences per device = None\n",
      "2023-07-24 18:21:46 | INFO | fairseq.trainer | Preparing to load checkpoint /workspace/process/LJ/checkpoint_last.pt\n",
      "2023-07-24 18:21:46 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
      "2023-07-24 18:21:47 | INFO | fairseq.trainer | Loaded checkpoint /workspace/process/LJ/checkpoint_last.pt (epoch 3 @ 64 updates)\n",
      "2023-07-24 18:21:47 | INFO | fairseq.trainer | loading train data for epoch 3\n",
      "2023-07-24 18:21:47 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2023-07-24 18:21:47 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': None}\n",
      "2023-07-24 18:21:47 | INFO | fairseq.data.audio.speech_to_text_dataset | 'train' has 0.00% OOV\n",
      "2023-07-24 18:21:47 | INFO | fairseq.data.audio.speech_to_text_dataset | TextToSpeechDataset(split=\"train\", n_samples=12_229, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    GlobalCMVN(stats_npz_path=\"/workspace/process/LJ/gcmvn_stats.npz\")\n",
      "), n_frames_per_step=4\n",
      "2023-07-24 18:21:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 18:21:48 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2023-07-24 18:21:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 18:22:35 | INFO | train_inner | epoch 003:     36 / 52 loss=6.294, l1_loss=2.144, mse_loss=4.003, eos_loss=0.148, attn_loss=0, ctc_loss=0, sample_size=33824.7, wps=26834.3, ups=0.79, wpb=33824.7, bsz=237.7, num_updates=100, lr=5e-05, gnorm=0, clip=0, train_wall=45, gb_free=18.1, wall=49\n",
      "2023-07-24 18:22:54 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 18:29:53 | INFO | dev | epoch 003 | valid on 'dev' subset | loss 4.052 | l1_loss 1.714 | mse_loss 2.202 | eos_loss 0.136 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 27.083 | pred_ratio 8.225 | ins_rate 7.239 | del_rate 0.014 | wps 117.3 | wpb 5645.1 | bsz 38.7 | num_updates 116 | best_mcd_loss 25.603\n",
      "2023-07-24 18:29:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 116 updates\n",
      "2023-07-24 18:29:53 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint3.pt\n",
      "2023-07-24 18:29:55 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint3.pt\n",
      "2023-07-24 18:30:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint3.pt (epoch 3 @ 116 updates, score 27.083) (writing took 16.12013516400475 seconds)\n",
      "2023-07-24 18:30:09 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2023-07-24 18:30:09 | INFO | train | epoch 003 | loss 6.267 | l1_loss 2.139 | mse_loss 3.981 | eos_loss 0.147 | attn_loss 0 | ctc_loss 0 | sample_size 33604.5 | wps 3394.4 | ups 0.1 | wpb 33190.1 | bsz 235.2 | num_updates 116 | lr 5.8e-05 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.6 | wall 504\n",
      "2023-07-24 18:30:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 18:30:10 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2023-07-24 18:30:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 18:31:15 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 18:38:22 | INFO | dev | epoch 004 | valid on 'dev' subset | loss 3.957 | l1_loss 1.695 | mse_loss 2.167 | eos_loss 0.095 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 29.379 | pred_ratio 8.225 | ins_rate 7.463 | del_rate 0.238 | wps 115.2 | wpb 5645.1 | bsz 38.7 | num_updates 168 | best_mcd_loss 25.603\n",
      "2023-07-24 18:38:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 168 updates\n",
      "2023-07-24 18:38:22 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint4.pt\n",
      "2023-07-24 18:38:24 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint4.pt\n",
      "2023-07-24 18:38:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint4.pt (epoch 4 @ 168 updates, score 29.379) (writing took 16.885769865009934 seconds)\n",
      "2023-07-24 18:38:39 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2023-07-24 18:38:39 | INFO | train | epoch 004 | loss 5.471 | l1_loss 2.002 | mse_loss 3.351 | eos_loss 0.118 | attn_loss 0 | ctc_loss 0 | sample_size 33598.1 | wps 3385.3 | ups 0.1 | wpb 33190.1 | bsz 235.2 | num_updates 168 | lr 8.4e-05 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.3 | wall 1013\n",
      "2023-07-24 18:38:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 18:38:39 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2023-07-24 18:38:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 18:39:21 | INFO | train_inner | epoch 005:     32 / 52 loss=5.414, l1_loss=1.988, mse_loss=3.313, eos_loss=0.113, attn_loss=0, ctc_loss=0, sample_size=33499.1, wps=3286.8, ups=0.1, wpb=33073.1, bsz=231.4, num_updates=200, lr=0.0001, gnorm=0, clip=0, train_wall=122, gb_free=18, wall=1055\n",
      "2023-07-24 18:39:45 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 18:46:18 | INFO | dev | epoch 005 | valid on 'dev' subset | loss 3.581 | l1_loss 1.587 | mse_loss 1.922 | eos_loss 0.072 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 31.206 | pred_ratio 7.61 | ins_rate 6.64 | del_rate 0.03 | wps 126.3 | wpb 5645.1 | bsz 38.7 | num_updates 220 | best_mcd_loss 25.603\n",
      "2023-07-24 18:46:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 220 updates\n",
      "2023-07-24 18:46:18 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint5.pt\n",
      "2023-07-24 18:46:20 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint5.pt\n",
      "2023-07-24 18:46:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint5.pt (epoch 5 @ 220 updates, score 31.206) (writing took 16.88831234292593 seconds)\n",
      "2023-07-24 18:46:34 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2023-07-24 18:46:34 | INFO | train | epoch 005 | loss 4.891 | l1_loss 1.884 | mse_loss 2.92 | eos_loss 0.087 | attn_loss 0 | ctc_loss 0 | sample_size 33609.2 | wps 3629.7 | ups 0.11 | wpb 33190.1 | bsz 235.2 | num_updates 220 | lr 0.00011 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.8 | wall 1489\n",
      "2023-07-24 18:46:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 18:46:35 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2023-07-24 18:46:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 18:47:41 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 18:52:55 | INFO | dev | epoch 006 | valid on 'dev' subset | loss 3.033 | l1_loss 1.423 | mse_loss 1.549 | eos_loss 0.06 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 24.815 | pred_ratio 5.269 | ins_rate 4.32 | del_rate 0.05 | wps 162.6 | wpb 5645.1 | bsz 38.7 | num_updates 272 | best_mcd_loss 24.815\n",
      "2023-07-24 18:52:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 272 updates\n",
      "2023-07-24 18:52:55 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint6.pt\n",
      "2023-07-24 18:52:58 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint6.pt\n",
      "2023-07-24 18:53:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint6.pt (epoch 6 @ 272 updates, score 24.815) (writing took 31.391740942024626 seconds)\n",
      "2023-07-24 18:53:27 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2023-07-24 18:53:27 | INFO | train | epoch 006 | loss 4.441 | l1_loss 1.756 | mse_loss 2.618 | eos_loss 0.067 | attn_loss 0 | ctc_loss 0 | sample_size 33604.5 | wps 4185.2 | ups 0.13 | wpb 33190.1 | bsz 235.2 | num_updates 272 | lr 0.000136 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.5 | wall 1901\n",
      "2023-07-24 18:53:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 18:53:27 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2023-07-24 18:53:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 18:54:04 | INFO | train_inner | epoch 007:     28 / 52 loss=4.433, l1_loss=1.753, mse_loss=2.611, eos_loss=0.068, attn_loss=0, ctc_loss=0, sample_size=33612.2, wps=3756.7, ups=0.11, wpb=33178.7, bsz=238.5, num_updates=300, lr=0.00015, gnorm=0, clip=0, train_wall=123, gb_free=18.1, wall=1939\n",
      "2023-07-24 18:54:33 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 18:57:15 | INFO | dev | epoch 007 | valid on 'dev' subset | loss 2.898 | l1_loss 1.384 | mse_loss 1.474 | eos_loss 0.04 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 12.895 | pred_ratio 1.89 | ins_rate 1.059 | del_rate 0.169 | wps 359.6 | wpb 5645.1 | bsz 38.7 | num_updates 324 | best_mcd_loss 12.895\n",
      "2023-07-24 18:57:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 324 updates\n",
      "2023-07-24 18:57:15 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint7.pt\n",
      "2023-07-24 18:57:17 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint7.pt\n",
      "2023-07-24 18:57:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint7.pt (epoch 7 @ 324 updates, score 12.895) (writing took 32.32241762697231 seconds)\n",
      "2023-07-24 18:57:47 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2023-07-24 18:57:47 | INFO | train | epoch 007 | loss 4.125 | l1_loss 1.664 | mse_loss 2.405 | eos_loss 0.057 | attn_loss 0 | ctc_loss 0 | sample_size 33597.5 | wps 6622.5 | ups 0.2 | wpb 33190.1 | bsz 235.2 | num_updates 324 | lr 0.000162 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.4 | wall 2162\n",
      "2023-07-24 18:57:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 18:57:48 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2023-07-24 18:57:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 18:58:54 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:01:44 | INFO | dev | epoch 008 | valid on 'dev' subset | loss 2.77 | l1_loss 1.333 | mse_loss 1.391 | eos_loss 0.046 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 13.838 | pred_ratio 2.097 | ins_rate 1.166 | del_rate 0.069 | wps 346.7 | wpb 5645.1 | bsz 38.7 | num_updates 376 | best_mcd_loss 12.895\n",
      "2023-07-24 19:01:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 376 updates\n",
      "2023-07-24 19:01:44 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint8.pt\n",
      "2023-07-24 19:01:46 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint8.pt\n",
      "2023-07-24 19:02:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint8.pt (epoch 8 @ 376 updates, score 13.838) (writing took 16.355693286051974 seconds)\n",
      "2023-07-24 19:02:00 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2023-07-24 19:02:00 | INFO | train | epoch 008 | loss 3.987 | l1_loss 1.622 | mse_loss 2.31 | eos_loss 0.055 | attn_loss 0 | ctc_loss 0 | sample_size 33605.6 | wps 6822.9 | ups 0.21 | wpb 33190.1 | bsz 235.2 | num_updates 376 | lr 0.000188 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.5 | wall 2415\n",
      "2023-07-24 19:02:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:02:01 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2023-07-24 19:02:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:02:32 | INFO | train_inner | epoch 009:     24 / 52 loss=3.982, l1_loss=1.621, mse_loss=2.309, eos_loss=0.051, attn_loss=0, ctc_loss=0, sample_size=33545.7, wps=6520.3, ups=0.2, wpb=33118.7, bsz=236, num_updates=400, lr=0.0002, gnorm=0, clip=0, train_wall=123, gb_free=18.2, wall=2447\n",
      "2023-07-24 19:03:06 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:07:05 | INFO | dev | epoch 009 | valid on 'dev' subset | loss 2.648 | l1_loss 1.295 | mse_loss 1.302 | eos_loss 0.05 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 19.294 | pred_ratio 3.507 | ins_rate 2.621 | del_rate 0.114 | wps 227 | wpb 5645.1 | bsz 38.7 | num_updates 428 | best_mcd_loss 12.895\n",
      "2023-07-24 19:07:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 428 updates\n",
      "2023-07-24 19:07:05 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint9.pt\n",
      "2023-07-24 19:07:07 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint9.pt\n",
      "2023-07-24 19:07:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint9.pt (epoch 9 @ 428 updates, score 19.294) (writing took 17.211096878978424 seconds)\n",
      "2023-07-24 19:07:22 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2023-07-24 19:07:22 | INFO | train | epoch 009 | loss 3.844 | l1_loss 1.578 | mse_loss 2.221 | eos_loss 0.045 | attn_loss 0 | ctc_loss 0 | sample_size 33602.1 | wps 5361.9 | ups 0.16 | wpb 33190.1 | bsz 235.2 | num_updates 428 | lr 0.000214 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.3 | wall 2737\n",
      "2023-07-24 19:07:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:07:23 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2023-07-24 19:07:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:08:28 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:10:39 | INFO | dev | epoch 010 | valid on 'dev' subset | loss 2.493 | l1_loss 1.234 | mse_loss 1.219 | eos_loss 0.04 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 9.361 | pred_ratio 1.277 | ins_rate 0.406 | del_rate 0.129 | wps 436.4 | wpb 5645.1 | bsz 38.7 | num_updates 480 | best_mcd_loss 9.361\n",
      "2023-07-24 19:10:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 480 updates\n",
      "2023-07-24 19:10:39 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint10.pt\n",
      "2023-07-24 19:10:42 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint10.pt\n",
      "2023-07-24 19:11:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint10.pt (epoch 10 @ 480 updates, score 9.361) (writing took 32.632201151922345 seconds)\n",
      "2023-07-24 19:11:12 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2023-07-24 19:11:12 | INFO | train | epoch 010 | loss 3.719 | l1_loss 1.538 | mse_loss 2.14 | eos_loss 0.042 | attn_loss 0 | ctc_loss 0 | sample_size 33594.7 | wps 7516 | ups 0.23 | wpb 33190.1 | bsz 235.2 | num_updates 480 | lr 0.00024 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.2 | wall 2966\n",
      "2023-07-24 19:11:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:11:12 | INFO | fairseq.trainer | begin training epoch 11\n",
      "2023-07-24 19:11:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:11:39 | INFO | train_inner | epoch 011:     20 / 52 loss=3.729, l1_loss=1.541, mse_loss=2.146, eos_loss=0.042, attn_loss=0, ctc_loss=0, sample_size=33711.8, wps=6081.6, ups=0.18, wpb=33285.3, bsz=234.3, num_updates=500, lr=0.00025, gnorm=0, clip=0, train_wall=123, gb_free=17.9, wall=2994\n",
      "2023-07-24 19:12:18 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:14:27 | INFO | dev | epoch 011 | valid on 'dev' subset | loss 2.487 | l1_loss 1.221 | mse_loss 1.223 | eos_loss 0.043 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 10.369 | pred_ratio 1.195 | ins_rate 0.336 | del_rate 0.141 | wps 437 | wpb 5645.1 | bsz 38.7 | num_updates 532 | best_mcd_loss 9.361\n",
      "2023-07-24 19:14:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 532 updates\n",
      "2023-07-24 19:14:27 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint11.pt\n",
      "2023-07-24 19:14:31 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint11.pt\n",
      "2023-07-24 19:14:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint11.pt (epoch 11 @ 532 updates, score 10.369) (writing took 18.708454807987437 seconds)\n",
      "2023-07-24 19:14:46 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
      "2023-07-24 19:14:46 | INFO | train | epoch 011 | loss 3.603 | l1_loss 1.498 | mse_loss 2.067 | eos_loss 0.038 | attn_loss 0 | ctc_loss 0 | sample_size 33615 | wps 8058.7 | ups 0.24 | wpb 33190.1 | bsz 235.2 | num_updates 532 | lr 0.000266 | gnorm 0 | clip 0 | train_wall 63 | gb_free 19 | wall 3181\n",
      "2023-07-24 19:14:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:14:46 | INFO | fairseq.trainer | begin training epoch 12\n",
      "2023-07-24 19:14:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:15:52 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:17:29 | INFO | dev | epoch 012 | valid on 'dev' subset | loss 2.44 | l1_loss 1.201 | mse_loss 1.195 | eos_loss 0.043 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 8.692 | pred_ratio 1.136 | ins_rate 0.255 | del_rate 0.119 | wps 519 | wpb 5645.1 | bsz 38.7 | num_updates 584 | best_mcd_loss 8.692\n",
      "2023-07-24 19:17:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 584 updates\n",
      "2023-07-24 19:17:29 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint12.pt\n",
      "2023-07-24 19:17:33 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint12.pt\n",
      "2023-07-24 19:18:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint12.pt (epoch 12 @ 584 updates, score 8.692) (writing took 32.011226237984374 seconds)\n",
      "2023-07-24 19:18:01 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
      "2023-07-24 19:18:01 | INFO | train | epoch 012 | loss 3.509 | l1_loss 1.464 | mse_loss 2.003 | eos_loss 0.041 | attn_loss 0 | ctc_loss 0 | sample_size 33593.5 | wps 8836.1 | ups 0.27 | wpb 33190.1 | bsz 235.2 | num_updates 584 | lr 0.000292 | gnorm 0 | clip 0 | train_wall 63 | gb_free 18.1 | wall 3376\n",
      "2023-07-24 19:18:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:18:02 | INFO | fairseq.trainer | begin training epoch 13\n",
      "2023-07-24 19:18:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:18:23 | INFO | train_inner | epoch 013:     16 / 52 loss=3.524, l1_loss=1.469, mse_loss=2.014, eos_loss=0.041, attn_loss=0, ctc_loss=0, sample_size=33565, wps=8212.9, ups=0.25, wpb=33134.9, bsz=231.9, num_updates=600, lr=0.0003, gnorm=0, clip=0, train_wall=121, gb_free=18, wall=3397\n",
      "2023-07-24 19:19:06 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:21:46 | INFO | dev | epoch 013 | valid on 'dev' subset | loss 2.299 | l1_loss 1.153 | mse_loss 1.112 | eos_loss 0.035 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 13.59 | pred_ratio 1.772 | ins_rate 0.93 | del_rate 0.159 | wps 364.9 | wpb 5645.1 | bsz 38.7 | num_updates 636 | best_mcd_loss 8.692\n",
      "2023-07-24 19:21:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 636 updates\n",
      "2023-07-24 19:21:46 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint13.pt\n",
      "2023-07-24 19:21:50 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint13.pt\n",
      "2023-07-24 19:22:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint13.pt (epoch 13 @ 636 updates, score 13.59) (writing took 18.701415837043896 seconds)\n",
      "2023-07-24 19:22:05 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
      "2023-07-24 19:22:05 | INFO | train | epoch 013 | loss 3.418 | l1_loss 1.433 | mse_loss 1.943 | eos_loss 0.041 | attn_loss 0 | ctc_loss 0 | sample_size 33600.6 | wps 7097 | ups 0.21 | wpb 33190.1 | bsz 235.2 | num_updates 636 | lr 0.000318 | gnorm 0 | clip 0 | train_wall 63 | gb_free 18.3 | wall 3619\n",
      "2023-07-24 19:22:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:22:05 | INFO | fairseq.trainer | begin training epoch 14\n",
      "2023-07-24 19:22:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:23:10 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:25:01 | INFO | dev | epoch 014 | valid on 'dev' subset | loss 2.29 | l1_loss 1.146 | mse_loss 1.117 | eos_loss 0.027 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 9.152 | pred_ratio 1.074 | ins_rate 0.216 | del_rate 0.141 | wps 447.2 | wpb 5645.1 | bsz 38.7 | num_updates 688 | best_mcd_loss 8.692\n",
      "2023-07-24 19:25:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 688 updates\n",
      "2023-07-24 19:25:01 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint14.pt\n",
      "2023-07-24 19:25:05 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint14.pt\n",
      "2023-07-24 19:25:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint14.pt (epoch 14 @ 688 updates, score 9.152) (writing took 19.096029510023072 seconds)\n",
      "2023-07-24 19:25:20 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
      "2023-07-24 19:25:20 | INFO | train | epoch 014 | loss 3.323 | l1_loss 1.404 | mse_loss 1.882 | eos_loss 0.036 | attn_loss 0 | ctc_loss 0 | sample_size 33600.8 | wps 8813.6 | ups 0.27 | wpb 33190.1 | bsz 235.2 | num_updates 688 | lr 0.000344 | gnorm 0 | clip 0 | train_wall 63 | gb_free 18.6 | wall 3815\n",
      "2023-07-24 19:25:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:25:21 | INFO | fairseq.trainer | begin training epoch 15\n",
      "2023-07-24 19:25:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:25:37 | INFO | train_inner | epoch 015:     12 / 52 loss=3.343, l1_loss=1.411, mse_loss=1.895, eos_loss=0.038, attn_loss=0, ctc_loss=0, sample_size=33656.2, wps=7645.5, ups=0.23, wpb=33228.4, bsz=240.2, num_updates=700, lr=0.00035, gnorm=0, clip=0, train_wall=122, gb_free=18.1, wall=3832\n",
      "2023-07-24 19:26:26 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:28:01 | INFO | dev | epoch 015 | valid on 'dev' subset | loss 2.253 | l1_loss 1.132 | mse_loss 1.087 | eos_loss 0.034 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 9.002 | pred_ratio 1.072 | ins_rate 0.17 | del_rate 0.098 | wps 529 | wpb 5645.1 | bsz 38.7 | num_updates 740 | best_mcd_loss 8.692\n",
      "2023-07-24 19:28:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 740 updates\n",
      "2023-07-24 19:28:01 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint15.pt\n",
      "2023-07-24 19:28:05 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint15.pt\n",
      "2023-07-24 19:28:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint15.pt (epoch 15 @ 740 updates, score 9.002) (writing took 18.817686937982216 seconds)\n",
      "2023-07-24 19:28:20 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
      "2023-07-24 19:28:20 | INFO | train | epoch 015 | loss 3.239 | l1_loss 1.379 | mse_loss 1.825 | eos_loss 0.035 | attn_loss 0 | ctc_loss 0 | sample_size 33601.5 | wps 9593.6 | ups 0.29 | wpb 33190.1 | bsz 235.2 | num_updates 740 | lr 0.00037 | gnorm 0 | clip 0 | train_wall 63 | gb_free 18.4 | wall 3995\n",
      "2023-07-24 19:28:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:28:21 | INFO | fairseq.trainer | begin training epoch 16\n",
      "2023-07-24 19:28:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:29:26 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:31:49 | INFO | dev | epoch 016 | valid on 'dev' subset | loss 2.075 | l1_loss 1.071 | mse_loss 0.977 | eos_loss 0.028 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 9.697 | pred_ratio 1.236 | ins_rate 0.355 | del_rate 0.118 | wps 393.7 | wpb 5645.1 | bsz 38.7 | num_updates 792 | best_mcd_loss 8.692\n",
      "2023-07-24 19:31:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 792 updates\n",
      "2023-07-24 19:31:49 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint16.pt\n",
      "2023-07-24 19:31:53 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint16.pt\n",
      "2023-07-24 19:32:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint16.pt (epoch 16 @ 792 updates, score 9.697) (writing took 19.676126357051544 seconds)\n",
      "2023-07-24 19:32:08 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
      "2023-07-24 19:32:08 | INFO | train | epoch 016 | loss 3.146 | l1_loss 1.349 | mse_loss 1.764 | eos_loss 0.033 | attn_loss 0 | ctc_loss 0 | sample_size 33601.6 | wps 7563.5 | ups 0.23 | wpb 33190.1 | bsz 235.2 | num_updates 792 | lr 0.000396 | gnorm 0 | clip 0 | train_wall 63 | gb_free 18.4 | wall 4223\n",
      "2023-07-24 19:32:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:32:09 | INFO | fairseq.trainer | begin training epoch 17\n",
      "2023-07-24 19:32:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:32:21 | INFO | train_inner | epoch 017:      8 / 52 loss=3.175, l1_loss=1.358, mse_loss=1.784, eos_loss=0.034, attn_loss=0, ctc_loss=0, sample_size=33602.5, wps=8224.7, ups=0.25, wpb=33174.6, bsz=233.5, num_updates=800, lr=0.0004, gnorm=0, clip=0, train_wall=122, gb_free=18, wall=4235\n",
      "2023-07-24 19:33:14 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:38:31 | INFO | dev | epoch 017 | valid on 'dev' subset | loss 2.383 | l1_loss 1.128 | mse_loss 1.155 | eos_loss 0.101 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 38.133 | pred_ratio 5.37 | ins_rate 4.455 | del_rate 0.084 | wps 161.2 | wpb 5645.1 | bsz 38.7 | num_updates 844 | best_mcd_loss 8.692\n",
      "2023-07-24 19:38:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 844 updates\n",
      "2023-07-24 19:38:31 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint17.pt\n",
      "2023-07-24 19:38:35 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint17.pt\n",
      "2023-07-24 19:38:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint17.pt (epoch 17 @ 844 updates, score 38.133) (writing took 19.099829437909648 seconds)\n",
      "2023-07-24 19:38:50 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
      "2023-07-24 19:38:50 | INFO | train | epoch 017 | loss 3.061 | l1_loss 1.321 | mse_loss 1.706 | eos_loss 0.033 | attn_loss 0 | ctc_loss 0 | sample_size 33608.7 | wps 4296.1 | ups 0.13 | wpb 33190.1 | bsz 235.2 | num_updates 844 | lr 0.000422 | gnorm 0 | clip 0 | train_wall 63 | gb_free 18.7 | wall 4625\n",
      "2023-07-24 19:38:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:38:51 | INFO | fairseq.trainer | begin training epoch 18\n",
      "2023-07-24 19:38:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:39:56 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:46:16 | INFO | dev | epoch 018 | valid on 'dev' subset | loss 2.128 | l1_loss 1.069 | mse_loss 1.008 | eos_loss 0.051 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 74.807 | pred_ratio 7.035 | ins_rate 6.11 | del_rate 0.075 | wps 130.5 | wpb 5645.1 | bsz 38.7 | num_updates 896 | best_mcd_loss 8.692\n",
      "2023-07-24 19:46:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 896 updates\n",
      "2023-07-24 19:46:16 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint18.pt\n",
      "2023-07-24 19:46:20 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint18.pt\n",
      "2023-07-24 19:46:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint18.pt (epoch 18 @ 896 updates, score 74.807) (writing took 18.579937753034756 seconds)\n",
      "2023-07-24 19:46:34 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
      "2023-07-24 19:46:34 | INFO | train | epoch 018 | loss 3.044 | l1_loss 1.318 | mse_loss 1.666 | eos_loss 0.059 | attn_loss 0 | ctc_loss 0 | sample_size 33596.8 | wps 3717.5 | ups 0.11 | wpb 33190.1 | bsz 235.2 | num_updates 896 | lr 0.000448 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.1 | wall 5089\n",
      "2023-07-24 19:46:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:46:35 | INFO | fairseq.trainer | begin training epoch 19\n",
      "2023-07-24 19:46:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:46:42 | INFO | train_inner | epoch 019:      4 / 52 loss=3.045, l1_loss=1.318, mse_loss=1.68, eos_loss=0.047, attn_loss=0, ctc_loss=0, sample_size=33572.5, wps=3849.6, ups=0.12, wpb=33143.8, bsz=234.9, num_updates=900, lr=0.00045, gnorm=0, clip=0, train_wall=122, gb_free=18.1, wall=5096\n",
      "2023-07-24 19:47:41 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:53:01 | INFO | dev | epoch 019 | valid on 'dev' subset | loss 2.33 | l1_loss 1.105 | mse_loss 1.175 | eos_loss 0.05 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 48.027 | pred_ratio 5.535 | ins_rate 4.715 | del_rate 0.18 | wps 159.8 | wpb 5645.1 | bsz 38.7 | num_updates 948 | best_mcd_loss 8.692\n",
      "2023-07-24 19:53:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 948 updates\n",
      "2023-07-24 19:53:01 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint19.pt\n",
      "2023-07-24 19:53:04 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint19.pt\n",
      "2023-07-24 19:53:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint19.pt (epoch 19 @ 948 updates, score 48.027) (writing took 18.734291812987067 seconds)\n",
      "2023-07-24 19:53:19 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
      "2023-07-24 19:53:19 | INFO | train | epoch 019 | loss 2.921 | l1_loss 1.282 | mse_loss 1.594 | eos_loss 0.045 | attn_loss 0 | ctc_loss 0 | sample_size 33598.1 | wps 4263.7 | ups 0.13 | wpb 33190.1 | bsz 235.2 | num_updates 948 | lr 0.000474 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.3 | wall 5494\n",
      "2023-07-24 19:53:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:53:20 | INFO | fairseq.trainer | begin training epoch 20\n",
      "2023-07-24 19:53:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:54:25 | INFO | train_inner | epoch 020:     52 / 52 loss=2.866, l1_loss=1.266, mse_loss=1.561, eos_loss=0.039, attn_loss=0, ctc_loss=0, sample_size=33570.6, wps=7147.1, ups=0.22, wpb=33145.1, bsz=235, num_updates=1000, lr=0.0005, gnorm=0, clip=0, train_wall=122, gb_free=18.6, wall=5560\n",
      "2023-07-24 19:54:25 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 19:56:59 | INFO | dev | epoch 020 | valid on 'dev' subset | loss 1.862 | l1_loss 0.987 | mse_loss 0.843 | eos_loss 0.032 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 10.6 | pred_ratio 1.528 | ins_rate 0.625 | del_rate 0.098 | wps 368.8 | wpb 5645.1 | bsz 38.7 | num_updates 1000 | best_mcd_loss 8.692\n",
      "2023-07-24 19:56:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1000 updates\n",
      "2023-07-24 19:56:59 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint20.pt\n",
      "2023-07-24 19:57:02 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint20.pt\n",
      "2023-07-24 19:57:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint20.pt (epoch 20 @ 1000 updates, score 10.6) (writing took 19.019792669918388 seconds)\n",
      "2023-07-24 19:57:18 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
      "2023-07-24 19:57:18 | INFO | train | epoch 020 | loss 2.82 | l1_loss 1.253 | mse_loss 1.533 | eos_loss 0.033 | attn_loss 0 | ctc_loss 0 | sample_size 33601.2 | wps 7238.2 | ups 0.22 | wpb 33190.1 | bsz 235.2 | num_updates 1000 | lr 0.0005 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.6 | wall 5732\n",
      "2023-07-24 19:57:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 19:57:18 | INFO | fairseq.trainer | begin training epoch 21\n",
      "2023-07-24 19:57:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 19:58:24 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 20:01:07 | INFO | dev | epoch 021 | valid on 'dev' subset | loss 1.953 | l1_loss 0.998 | mse_loss 0.914 | eos_loss 0.041 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 10.571 | pred_ratio 1.407 | ins_rate 0.511 | del_rate 0.104 | wps 329.8 | wpb 5645.1 | bsz 38.7 | num_updates 1052 | best_mcd_loss 8.692\n",
      "2023-07-24 20:01:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1052 updates\n",
      "2023-07-24 20:01:07 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint21.pt\n",
      "2023-07-24 20:01:11 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint21.pt\n",
      "2023-07-24 20:01:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint21.pt (epoch 21 @ 1052 updates, score 10.571) (writing took 19.24971270095557 seconds)\n",
      "2023-07-24 20:01:26 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
      "2023-07-24 20:01:26 | INFO | train | epoch 021 | loss 2.732 | l1_loss 1.226 | mse_loss 1.473 | eos_loss 0.033 | attn_loss 0 | ctc_loss 0 | sample_size 33603.1 | wps 6952.7 | ups 0.21 | wpb 33190.1 | bsz 235.2 | num_updates 1052 | lr 0.000526 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.6 | wall 5980\n",
      "2023-07-24 20:01:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 20:01:26 | INFO | fairseq.trainer | begin training epoch 22\n",
      "2023-07-24 20:01:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 20:02:28 | INFO | train_inner | epoch 022:     48 / 52 loss=2.693, l1_loss=1.215, mse_loss=1.444, eos_loss=0.033, attn_loss=0, ctc_loss=0, sample_size=33641, wps=6923.1, ups=0.21, wpb=33426, bsz=236.9, num_updates=1100, lr=0.00055, gnorm=0, clip=0, train_wall=124, gb_free=18.1, wall=6043\n",
      "2023-07-24 20:02:32 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 20:04:14 | INFO | dev | epoch 022 | valid on 'dev' subset | loss 1.975 | l1_loss 1.016 | mse_loss 0.934 | eos_loss 0.025 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 9.785 | pred_ratio 1.275 | ins_rate 0.414 | del_rate 0.139 | wps 495.8 | wpb 5645.1 | bsz 38.7 | num_updates 1104 | best_mcd_loss 8.692\n",
      "2023-07-24 20:04:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1104 updates\n",
      "2023-07-24 20:04:14 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint22.pt\n",
      "2023-07-24 20:04:18 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint22.pt\n",
      "2023-07-24 20:04:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint22.pt (epoch 22 @ 1104 updates, score 9.785) (writing took 18.416464170906693 seconds)\n",
      "2023-07-24 20:04:32 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
      "2023-07-24 20:04:32 | INFO | train | epoch 022 | loss 2.65 | l1_loss 1.204 | mse_loss 1.412 | eos_loss 0.034 | attn_loss 0 | ctc_loss 0 | sample_size 33593.7 | wps 9255.8 | ups 0.28 | wpb 33190.1 | bsz 235.2 | num_updates 1104 | lr 0.000552 | gnorm 0 | clip 0 | train_wall 64 | gb_free 17.9 | wall 6167\n",
      "2023-07-24 20:04:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 20:04:33 | INFO | fairseq.trainer | begin training epoch 23\n",
      "2023-07-24 20:04:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 20:05:39 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 20:07:31 | INFO | dev | epoch 023 | valid on 'dev' subset | loss 1.816 | l1_loss 0.958 | mse_loss 0.827 | eos_loss 0.03 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 9.032 | pred_ratio 1.154 | ins_rate 0.28 | del_rate 0.125 | wps 515.8 | wpb 5645.1 | bsz 38.7 | num_updates 1156 | best_mcd_loss 8.692\n",
      "2023-07-24 20:07:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1156 updates\n",
      "2023-07-24 20:07:31 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint23.pt\n",
      "2023-07-24 20:07:35 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint23.pt\n",
      "2023-07-24 20:07:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint23.pt (epoch 23 @ 1156 updates, score 9.032) (writing took 18.487467429949902 seconds)\n",
      "2023-07-24 20:07:50 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
      "2023-07-24 20:07:50 | INFO | train | epoch 023 | loss 2.572 | l1_loss 1.184 | mse_loss 1.356 | eos_loss 0.032 | attn_loss 0 | ctc_loss 0 | sample_size 33598.5 | wps 8738.7 | ups 0.26 | wpb 33190.1 | bsz 235.2 | num_updates 1156 | lr 0.000578 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.4 | wall 6364\n",
      "2023-07-24 20:07:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 20:07:50 | INFO | fairseq.trainer | begin training epoch 24\n",
      "2023-07-24 20:07:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 20:08:47 | INFO | train_inner | epoch 024:     44 / 52 loss=2.534, l1_loss=1.173, mse_loss=1.33, eos_loss=0.031, attn_loss=0, ctc_loss=0, sample_size=33667.7, wps=8770.3, ups=0.26, wpb=33244.3, bsz=236.5, num_updates=1200, lr=0.0006, gnorm=0, clip=0, train_wall=123, gb_free=18.1, wall=6422\n",
      "2023-07-24 20:08:56 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 20:12:21 | INFO | dev | epoch 024 | valid on 'dev' subset | loss 1.881 | l1_loss 0.956 | mse_loss 0.854 | eos_loss 0.072 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 14.111 | pred_ratio 2.714 | ins_rate 1.757 | del_rate 0.043 | wps 271.3 | wpb 5645.1 | bsz 38.7 | num_updates 1208 | best_mcd_loss 8.692\n",
      "2023-07-24 20:12:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1208 updates\n",
      "2023-07-24 20:12:21 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint24.pt\n",
      "2023-07-24 20:12:25 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint24.pt\n",
      "2023-07-24 20:12:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint24.pt (epoch 24 @ 1208 updates, score 14.111) (writing took 18.671980264945887 seconds)\n",
      "2023-07-24 20:12:39 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
      "2023-07-24 20:12:39 | INFO | train | epoch 024 | loss 2.482 | l1_loss 1.158 | mse_loss 1.294 | eos_loss 0.03 | attn_loss 0 | ctc_loss 0 | sample_size 33607.5 | wps 5963.6 | ups 0.18 | wpb 33190.1 | bsz 235.2 | num_updates 1208 | lr 0.000604 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.6 | wall 6654\n",
      "2023-07-24 20:12:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 20:12:40 | INFO | fairseq.trainer | begin training epoch 25\n",
      "2023-07-24 20:12:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 20:13:46 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 20:16:44 | INFO | dev | epoch 025 | valid on 'dev' subset | loss 1.799 | l1_loss 0.941 | mse_loss 0.79 | eos_loss 0.069 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 11.523 | pred_ratio 1.957 | ins_rate 1.033 | del_rate 0.076 | wps 310.1 | wpb 5645.1 | bsz 38.7 | num_updates 1260 | best_mcd_loss 8.692\n",
      "2023-07-24 20:16:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1260 updates\n",
      "2023-07-24 20:16:44 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint25.pt\n",
      "2023-07-24 20:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint25.pt\n",
      "2023-07-24 20:17:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint25.pt (epoch 25 @ 1260 updates, score 11.523) (writing took 18.43264467502013 seconds)\n",
      "2023-07-24 20:17:03 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
      "2023-07-24 20:17:03 | INFO | train | epoch 025 | loss 2.417 | l1_loss 1.142 | mse_loss 1.244 | eos_loss 0.032 | attn_loss 0 | ctc_loss 0 | sample_size 33610.9 | wps 6553 | ups 0.2 | wpb 33190.1 | bsz 235.2 | num_updates 1260 | lr 0.00063 | gnorm 0 | clip 0 | train_wall 64 | gb_free 18.9 | wall 6917\n",
      "2023-07-24 20:17:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 20:17:03 | INFO | fairseq.trainer | begin training epoch 26\n",
      "2023-07-24 20:17:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 20:17:56 | INFO | train_inner | epoch 026:     40 / 52 loss=2.392, l1_loss=1.135, mse_loss=1.226, eos_loss=0.032, attn_loss=0, ctc_loss=0, sample_size=33603, wps=6049, ups=0.18, wpb=33167.3, bsz=236.3, num_updates=1300, lr=0.00065, gnorm=0, clip=0, train_wall=123, gb_free=18.1, wall=6970\n",
      "2023-07-24 20:18:10 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 20:22:29 | INFO | dev | epoch 026 | valid on 'dev' subset | loss 1.696 | l1_loss 0.913 | mse_loss 0.748 | eos_loss 0.035 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 20.467 | pred_ratio 3.774 | ins_rate 2.853 | del_rate 0.08 | wps 204 | wpb 5645.1 | bsz 38.7 | num_updates 1312 | best_mcd_loss 8.692\n",
      "2023-07-24 20:22:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1312 updates\n",
      "2023-07-24 20:22:29 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint26.pt\n",
      "2023-07-24 20:22:31 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint26.pt\n",
      "2023-07-24 20:22:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint26.pt (epoch 26 @ 1312 updates, score 20.467) (writing took 16.410342588089406 seconds)\n",
      "2023-07-24 20:22:46 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
      "2023-07-24 20:22:46 | INFO | train | epoch 026 | loss 2.343 | l1_loss 1.122 | mse_loss 1.19 | eos_loss 0.032 | attn_loss 0 | ctc_loss 0 | sample_size 33604.2 | wps 5034.3 | ups 0.15 | wpb 33190.1 | bsz 235.2 | num_updates 1312 | lr 0.000656 | gnorm 0 | clip 0 | train_wall 65 | gb_free 18.4 | wall 7260\n",
      "2023-07-24 20:22:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 20:22:46 | INFO | fairseq.trainer | begin training epoch 27\n",
      "2023-07-24 20:22:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-07-24 20:23:52 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
      "2023-07-24 20:28:32 | INFO | dev | epoch 027 | valid on 'dev' subset | loss 1.677 | l1_loss 0.904 | mse_loss 0.712 | eos_loss 0.06 | attn_loss 0 | ctc_loss 0 | sample_size 5645.11 | mcd_loss 22.465 | pred_ratio 4.645 | ins_rate 3.675 | del_rate 0.031 | wps 186.9 | wpb 5645.1 | bsz 38.7 | num_updates 1364 | best_mcd_loss 8.692\n",
      "2023-07-24 20:28:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1364 updates\n",
      "2023-07-24 20:28:32 | INFO | fairseq.trainer | Saving checkpoint to /workspace/process/LJ/checkpoint27.pt\n",
      "2023-07-24 20:28:34 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/process/LJ/checkpoint27.pt\n",
      "2023-07-24 20:28:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /workspace/process/LJ/checkpoint27.pt (epoch 27 @ 1364 updates, score 22.465) (writing took 16.27546767797321 seconds)\n",
      "2023-07-24 20:28:48 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
      "2023-07-24 20:28:48 | INFO | train | epoch 027 | loss 2.265 | l1_loss 1.104 | mse_loss 1.133 | eos_loss 0.028 | attn_loss 0 | ctc_loss 0 | sample_size 33600.2 | wps 4762.8 | ups 0.14 | wpb 33190.1 | bsz 235.2 | num_updates 1364 | lr 0.000682 | gnorm 0 | clip 0 | train_wall 65 | gb_free 18.3 | wall 7622\n",
      "2023-07-24 20:28:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 52\n",
      "2023-07-24 20:28:48 | INFO | fairseq.trainer | begin training epoch 28\n",
      "2023-07-24 20:28:48 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m             distributed_utils\u001b[38;5;241m.\u001b[39mcall_main(cfg, main)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mdistributed_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq/distributed/utils.py:369\u001b[0m, in \u001b[0;36mcall_main\u001b[0;34m(cfg, main, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m     xmp\u001b[38;5;241m.\u001b[39mspawn(\n\u001b[1;32m    360\u001b[0m         fn\u001b[38;5;241m=\u001b[39mdistributed_main,\n\u001b[1;32m    361\u001b[0m         args\u001b[38;5;241m=\u001b[39m(main, cfg, kwargs),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m         nprocs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mdistributed_training\u001b[38;5;241m.\u001b[39mdistributed_world_size, \u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m    366\u001b[0m     )\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# single GPU main\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq_cli/train.py:190\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# train for one epoch\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m valid_losses, should_stop \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_itr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_stop:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq_cli/train.py:316\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(cfg, trainer, task, epoch_itr)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress):\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39maggregate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m), torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_step-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m i\n\u001b[1;32m    315\u001b[0m     ):\n\u001b[0;32m--> 316\u001b[0m         log_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m log_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# not OOM, overflow, ...\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;66;03m# log mid-epoch stats\u001b[39;00m\n\u001b[1;32m    320\u001b[0m         num_updates \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mget_num_updates()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq/trainer.py:824\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, samples, raise_oom)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m maybe_no_sync():\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;66;03m# forward and backward\u001b[39;00m\n\u001b[0;32m--> 824\u001b[0m         loss, sample_size_i, logging_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mupdate_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_updates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_dummy_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m loss\n\u001b[1;32m    835\u001b[0m     logging_outputs\u001b[38;5;241m.\u001b[39mappend(logging_output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq/tasks/fairseq_task.py:519\u001b[0m, in \u001b[0;36mFairseqTask.train_step\u001b[0;34m(self, sample, model, criterion, optimizer, update_num, ignore_grad)\u001b[0m\n\u001b[1;32m    517\u001b[0m     loss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 519\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, sample_size, logging_output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq/optim/fairseq_optimizer.py:95\u001b[0m, in \u001b[0;36mFairseqOptimizer.backward\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss):\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the sum of gradients of the given tensor w.r.t. graph leaves.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg = train.convert_namespace_to_omegaconf(args)\n",
    "\n",
    "if cfg.common.use_plasma_view:\n",
    "    server = PlasmaStore(path=cfg.common.plasma_path)\n",
    "    logger.info(\n",
    "        f\"Started plasma server pid {server.server.pid} {cfg.common.plasma_path}\"\n",
    "    )\n",
    "\n",
    "if args.profile:\n",
    "    with torch.cuda.profiler.profile():\n",
    "        with torch.autograd.profiler.emit_nvtx():\n",
    "            distributed_utils.call_main(cfg, main)\n",
    "else:\n",
    "    distributed_utils.call_main(cfg, train.main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4132683f-9548-4e83-bf68-f9c62d8d1922",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd7f77a7-4d0f-47d3-bb20-75f0f23da01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='text_to_speech', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=50000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=50000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, path='/workspace/production/LJ/checkpoint_avg_last_5.pt', post_process=None, quiet=False, model_overrides='{}', results_path='/workspace/production/LJ/results', eos_prob_threshold=0.5, dump_features=False, dump_waveforms=True, dump_attentions=False, dump_eos_probs=False, dump_plots=False, dump_target=True, output_sample_rate=22050, teacher_forcing=False, audio_format='wav', data='/workspace/production/LJ/', config_yaml='config.yaml', max_source_positions=1024, max_target_positions=1200, n_frames_per_step=1, eval_inference=False, eval_tb_nsample=8, vocoder='hifigan', spec_bwd_max_iter=32, force_anneal=None, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3, no_seed_provided=False)\n"
     ]
    }
   ],
   "source": [
    "waveformParser = generate_waveform.make_parser()\n",
    "args = options.parse_args_and_arch(waveformParser, [out_path, '--config-yaml', 'config.yaml', '--gen-subset', 'test', '--task', 'text_to_speech', '--path', out_path+'checkpoint_avg_last_5.pt', '--max-tokens', '50000', '--spec-bwd-max-iter', '32', '--dump-waveforms', '--results-path', out_path+'results', '--dump-target', '--vocoder', 'hifigan'])\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46630ef7-a3dd-4cbe-87c2-4a385b2b1997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 21:36:33 | INFO | fairseq.examples.speech_synthesis.generate_waveform | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='text_to_speech', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=50000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=50000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, path='/workspace/production/LJ/checkpoint_avg_last_5.pt', post_process=None, quiet=False, model_overrides='{}', results_path='/workspace/production/LJ/results', eos_prob_threshold=0.5, dump_features=False, dump_waveforms=True, dump_attentions=False, dump_eos_probs=False, dump_plots=False, dump_target=True, output_sample_rate=22050, teacher_forcing=False, audio_format='wav', data='/workspace/production/LJ/', config_yaml='config.yaml', max_source_positions=1024, max_target_positions=1200, n_frames_per_step=1, eval_inference=False, eval_tb_nsample=8, vocoder='hifigan', spec_bwd_max_iter=32, force_anneal=None, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3, no_seed_provided=False)\n",
      "2023-07-24 21:36:33 | INFO | fairseq.tasks.speech_to_text | dictionary size (vocab.txt): 75\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_waveform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq/examples/speech_synthesis/generate_waveform.py:135\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    133\u001b[0m use_cuda \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcpu\n\u001b[1;32m    134\u001b[0m task \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39msetup_task(args)\n\u001b[0;32m--> 135\u001b[0m models, saved_cfg, task \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_ensemble_and_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_overrides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m model \u001b[38;5;241m=\u001b[39m models[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m use_cuda \u001b[38;5;28;01melse\u001b[39;00m models[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# use the original n_frames_per_step\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq/checkpoint_utils.py:473\u001b[0m, in \u001b[0;36mload_model_ensemble_and_task\u001b[0;34m(filenames, arg_overrides, task, strict, suffix, num_shards, state)\u001b[0m\n\u001b[1;32m    471\u001b[0m argspec \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetfullargspec(task\u001b[38;5;241m.\u001b[39mbuild_model)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m argspec\u001b[38;5;241m.\u001b[39margs:\n\u001b[0;32m--> 473\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    475\u001b[0m     model \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mbuild_model(cfg\u001b[38;5;241m.\u001b[39mmodel)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq/tasks/text_to_speech.py:145\u001b[0m, in \u001b[0;36mTextToSpeechTask.build_model\u001b[0;34m(self, cfg, from_checkpoint)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(cfg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq/tasks/text_to_speech.py:150\u001b[0m, in \u001b[0;36mTextToSpeechTask.build_generator\u001b[0;34m(self, models, cfg, vocoder, **unused)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_generator\u001b[39m(\u001b[38;5;28mself\u001b[39m, models, cfg, vocoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused):\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vocoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m         vocoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_default_vocoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     model \u001b[38;5;241m=\u001b[39m models[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNON_AUTOREGRESSIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq/tasks/text_to_speech.py:170\u001b[0m, in \u001b[0;36mTextToSpeechTask.build_default_vocoder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_default_vocoder\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_to_speech\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_vocoder\n\u001b[0;32m--> 170\u001b[0m     vocoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_vocoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mcpu:\n\u001b[1;32m    172\u001b[0m         vocoder \u001b[38;5;241m=\u001b[39m vocoder\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq/models/text_to_speech/vocoder.py:255\u001b[0m, in \u001b[0;36mget_vocoder\u001b[0;34m(args, data_cfg)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GriffinLimVocoder\u001b[38;5;241m.\u001b[39mfrom_data_cfg(args, data_cfg)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mvocoder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhifigan\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHiFiGANVocoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mvocoder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_hifigan\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CodeHiFiGANVocoder\u001b[38;5;241m.\u001b[39mfrom_data_cfg(args, data_cfg)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fairseq/models/text_to_speech/vocoder.py:208\u001b[0m, in \u001b[0;36mHiFiGANVocoder.from_data_cfg\u001b[0;34m(cls, args, data_cfg)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_cfg\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args, data_cfg: S2TDataConfig):\n\u001b[1;32m    207\u001b[0m     vocoder_cfg \u001b[38;5;241m=\u001b[39m data_cfg\u001b[38;5;241m.\u001b[39mvocoder\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m vocoder_cfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgriffin_lim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhifigan\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(vocoder_cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    210\u001b[0m         model_cfg \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_waveform.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c53033b6-d3a1-487c-ba7c-129f612616f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocoder_cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvocoder_cfg\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgriffin_lim\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocoder_cfg' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233af11e-f26e-4d35-bd5e-69c538b30f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
