{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04415985-e3eb-4310-9a90-c0f308006da2",
   "metadata": {},
   "source": [
    "# PIP INSTALLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5696370d-ab0d-44b0-af72-8859d641623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n",
      "Requirement already satisfied: soundfile in /opt/conda/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: librosa==0.9.1 in /opt/conda/lib/python3.10/site-packages (0.9.1)\n",
      "Requirement already satisfied: g2p_en in /opt/conda/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: audioread>=2.1.5 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (1.24.3)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (0.4.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (1.3.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (1.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (23.0)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (1.6.0)\n",
      "Requirement already satisfied: numba>=0.45.1 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (0.57.1)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1) (5.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: protobuf>=4.22.3 in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (4.23.4)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: inflect>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from g2p_en) (7.0.0)\n",
      "Requirement already satisfied: distance>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from g2p_en) (0.1.3)\n",
      "Requirement already satisfied: nltk>=3.2.4 in /opt/conda/lib/python3.10/site-packages (from g2p_en) (3.8.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from inflect>=0.3.1->g2p_en) (4.7.1)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from inflect>=0.3.1->g2p_en) (2.0.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.2.4->g2p_en) (8.1.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk>=3.2.4->g2p_en) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.2.4->g2p_en) (2023.6.3)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.45.1->librosa==0.9.1) (0.40.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.1) (2.29.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.1) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->librosa==0.9.1) (3.2.0)\n",
      "Requirement already satisfied: pydantic-core==2.3.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9.1->inflect>=0.3.1->g2p_en) (2.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9.1->inflect>=0.3.1->g2p_en) (0.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (2023.5.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas tensorboardX sentencepiece soundfile librosa==0.9.1 g2p_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5bfac-4c6d-4fd1-8fc9-2f1b9d646c9e",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152d47d2-d7a5-45d3-ae5e-c42e447a4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Union\n",
    "\n",
    "from typing import Optional, List, Dict\n",
    "import zipfile\n",
    "import tempfile\n",
    "from dataclasses import dataclass\n",
    "from itertools import groupby\n",
    "\n",
    "import torchaudio\n",
    "from torch import Tensor\n",
    "from torch.hub import download_url_to_file\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio.datasets.utils import _extract_tar\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import shutil\n",
    "from tempfile import NamedTemporaryFile\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from fairseq.examples.speech_to_text.data_utils import save_df_to_tsv, create_zip, gen_config_yaml, gen_vocab, get_zip_manifest, load_tsv_to_dicts\n",
    "from fairseq.data.audio.audio_utils import convert_waveform, TTSSpectrogram, TTSMelScale, parse_path, read_from_stored_zip, is_npy_data\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "SPLITS = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "out_path = '/workspace/process/LJ/'\n",
    "in_path = '/workspace/LJSpeech-1.1/wavs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19c1d6-8ff4-4592-a8c7-542c44790f61",
   "metadata": {},
   "source": [
    "# DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd588ff-6a18-491f-b6d3-741bc4889de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LJSPEECH(Dataset):\n",
    "    \"\"\"*LJSpeech-1.1* :cite:`ljspeech17` dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str or Path): Path to the directory where the dataset is found or downloaded.\n",
    "        url (str, optional): The URL to download the dataset from.\n",
    "            (default: ``\"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"``)\n",
    "        folder_in_archive (str, optional):\n",
    "            The top-level directory of the dataset. (default: ``\"wavs\"``)\n",
    "        download (bool, optional):\n",
    "            Whether to download the dataset if it is not found at root path. (default: ``False``).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        self._parse_filesystem()\n",
    "\n",
    "    def _parse_filesystem(self) -> None:\n",
    "        root = '/workspace/LJSpeech-1.1'\n",
    "\n",
    "        self._path = root + '/wavs'\n",
    "        self._metadata_path = root + \"/metadata.csv\"\n",
    "\n",
    "        with open(self._metadata_path, \"r\", newline=\"\") as metadata:\n",
    "            flist = csv.reader(metadata, delimiter=\"|\", quoting=csv.QUOTE_NONE)\n",
    "            self._flist = list(flist)\n",
    "\n",
    "    def __getitem__(self, n: int) -> Tuple[Tensor, int, str, str]:\n",
    "        \"\"\"Load the n-th sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            n (int): The index of the sample to be loaded\n",
    "\n",
    "        Returns:\n",
    "            Tuple of the following items;\n",
    "\n",
    "            Tensor:\n",
    "                Waveform\n",
    "            int:\n",
    "                Sample rate\n",
    "            str:\n",
    "                Transcript\n",
    "            str:\n",
    "                Normalized Transcript\n",
    "        \"\"\"\n",
    "        line = self._flist[n]\n",
    "        fileid, transcript, normalized_transcript = line\n",
    "        fileid_audio = self._path +'/'+(fileid + \".wav\")\n",
    "\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(fileid_audio)\n",
    "\n",
    "        return (\n",
    "            waveform,\n",
    "            sample_rate,\n",
    "            transcript,\n",
    "            normalized_transcript,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._flist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eafc05-98aa-454c-a47c-b485e66a3a6b",
   "metadata": {},
   "source": [
    "# Audio Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40fe74c-8408-4a6e-92d7-fde39319fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAudioManifest():\n",
    "    out_root = Path(out_path).absolute()\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Generate TSV manifest\n",
    "    print(\"Generating manifest...\")\n",
    "    # following FastSpeech's splits\n",
    "    dataset = LJSPEECH()\n",
    "    id_to_split = {}\n",
    "    for x in dataset._flist:\n",
    "        id_ = x[0]\n",
    "        speaker = id_.split(\"-\")[0]\n",
    "        id_to_split[id_] = {\n",
    "            \"LJ001\": \"test\", \"LJ002\": \"test\", \"LJ003\": \"dev\"\n",
    "        }.get(speaker, \"train\")\n",
    "    manifest_by_split = {split: defaultdict(list) for split in SPLITS}\n",
    "    progress = tqdm(enumerate(dataset), total=len(dataset))\n",
    "    for i, (waveform, _, utt, normalized_utt) in progress:\n",
    "        sample_id = dataset._flist[i][0]\n",
    "        split = id_to_split[sample_id]\n",
    "        manifest_by_split[split][\"id\"].append(sample_id)\n",
    "        audio_path = f\"{dataset._path}/{sample_id}.wav\"\n",
    "        manifest_by_split[split][\"audio\"].append(audio_path)\n",
    "        manifest_by_split[split][\"n_frames\"].append(len(waveform[0]))\n",
    "        manifest_by_split[split][\"tgt_text\"].append(normalized_utt)\n",
    "        manifest_by_split[split][\"speaker\"].append(\"ljspeech\")\n",
    "        manifest_by_split[split][\"src_text\"].append(utt)\n",
    "\n",
    "    manifest_root = Path(out_path).absolute()\n",
    "    manifest_root.mkdir(parents=True, exist_ok=True)\n",
    "    for split in SPLITS:\n",
    "        save_df_to_tsv(\n",
    "            pd.DataFrame.from_dict(manifest_by_split[split]),\n",
    "            manifest_root / f\"{split}.audio.tsv\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc4ae7c-74ef-478b-80df-7ffa3a3ecb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating manifest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13100/13100 [00:57<00:00, 228.39it/s]\n"
     ]
    }
   ],
   "source": [
    "processAudioManifest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05311e2-d38a-48cd-8fab-9192555de95c",
   "metadata": {},
   "source": [
    "# Feature Manifest Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d53b1a-0d7d-4303-84b8-c773792c7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_or_pad_to_target_length(\n",
    "        data_1d_or_2d: np.ndarray, target_length: int\n",
    ") -> np.ndarray:\n",
    "    assert len(data_1d_or_2d.shape) in {1, 2}\n",
    "    delta = data_1d_or_2d.shape[0] - target_length\n",
    "    if delta >= 0:  # trim if being longer\n",
    "        data_1d_or_2d = data_1d_or_2d[: target_length]\n",
    "    else:  # pad if being shorter\n",
    "        if len(data_1d_or_2d.shape) == 1:\n",
    "            data_1d_or_2d = np.concatenate(\n",
    "                [data_1d_or_2d, np.zeros(-delta)], axis=0\n",
    "            )\n",
    "        else:\n",
    "            data_1d_or_2d = np.concatenate(\n",
    "                [data_1d_or_2d, np.zeros((-delta, data_1d_or_2d.shape[1]))],\n",
    "                axis=0\n",
    "            )\n",
    "    return data_1d_or_2d\n",
    "\n",
    "\n",
    "def extract_logmel_spectrogram(\n",
    "        waveform: torch.Tensor, sample_rate: int,\n",
    "        output_path: Optional[Path] = None, win_length: int = 1024,\n",
    "        hop_length: int = 256, n_fft: int = 1024,\n",
    "        win_fn: callable = torch.hann_window, n_mels: int = 80,\n",
    "        f_min: float = 0., f_max: float = 8000, eps: float = 1e-5,\n",
    "        overwrite: bool = False, target_length: Optional[int] = None\n",
    "):\n",
    "    if output_path is not None and output_path.is_file() and not overwrite:\n",
    "        return\n",
    "\n",
    "    spectrogram_transform = TTSSpectrogram(\n",
    "        n_fft=n_fft, win_length=win_length, hop_length=hop_length,\n",
    "        window_fn=win_fn\n",
    "    )\n",
    "    mel_scale_transform = TTSMelScale(\n",
    "        n_mels=n_mels, sample_rate=sample_rate, f_min=f_min, f_max=f_max,\n",
    "        n_stft=n_fft // 2 + 1\n",
    "    )\n",
    "    spectrogram = spectrogram_transform(waveform)\n",
    "    mel_spec = mel_scale_transform(spectrogram)\n",
    "    logmel_spec = torch.clamp(mel_spec, min=eps).log()\n",
    "    assert len(logmel_spec.shape) == 3 and logmel_spec.shape[0] == 1\n",
    "    logmel_spec = logmel_spec.squeeze().t()  # D x T -> T x D\n",
    "    if target_length is not None:\n",
    "        logmel_spec = trim_or_pad_to_target_length(logmel_spec, target_length)\n",
    "\n",
    "    if output_path is not None:\n",
    "        np.save(output_path.as_posix(), logmel_spec)\n",
    "    else:\n",
    "        return logmel_spec\n",
    "\n",
    "\n",
    "def extract_pitch(\n",
    "        waveform: torch.Tensor, sample_rate: int,\n",
    "        output_path: Optional[Path] = None, hop_length: int = 256,\n",
    "        log_scale: bool = True, phoneme_durations: Optional[List[int]] = None\n",
    "):\n",
    "    if output_path is not None and output_path.is_file():\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        import pyworld\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install PyWORLD: pip install pyworld\")\n",
    "\n",
    "    _waveform = waveform.squeeze(0).double().numpy()\n",
    "    pitch, t = pyworld.dio(\n",
    "        _waveform, sample_rate, frame_period=hop_length / sample_rate * 1000\n",
    "    )\n",
    "    pitch = pyworld.stonemask(_waveform, pitch, t, sample_rate)\n",
    "\n",
    "    if phoneme_durations is not None:\n",
    "        pitch = trim_or_pad_to_target_length(pitch, sum(phoneme_durations))\n",
    "        try:\n",
    "            from scipy.interpolate import interp1d\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install SciPy: pip install scipy\")\n",
    "        nonzero_ids = np.where(pitch != 0)[0]\n",
    "        if len(nonzero_ids) == 0:\n",
    "            print((f\"{output_path} has all empty values in the pitch contour\"))\n",
    "            return\n",
    "        elif len(nonzero_ids) == 1:\n",
    "            print((f\"{output_path} has only one non-zero values in the pitch contour\"))\n",
    "            return\n",
    "        else:\n",
    "            interp_fn = interp1d(\n",
    "                nonzero_ids,\n",
    "                pitch[nonzero_ids],\n",
    "                fill_value=(pitch[nonzero_ids[0]], pitch[nonzero_ids[-1]]),\n",
    "                bounds_error=False,\n",
    "            )\n",
    "            pitch = interp_fn(np.arange(0, len(pitch)))\n",
    "        d_cumsum = np.cumsum(np.concatenate([np.array([0]), phoneme_durations]))\n",
    "        pitch = np.array(\n",
    "            [\n",
    "                np.mean(pitch[d_cumsum[i-1]: d_cumsum[i]])\n",
    "                for i in range(1, len(d_cumsum))\n",
    "            ]\n",
    "        )\n",
    "        assert len(pitch) == len(phoneme_durations)\n",
    "\n",
    "    if log_scale:\n",
    "        pitch = np.log(pitch + 1)\n",
    "\n",
    "    if output_path is not None:\n",
    "        np.save(output_path.as_posix(), pitch)\n",
    "    else:\n",
    "        return pitch\n",
    "\n",
    "\n",
    "def extract_energy(\n",
    "        waveform: torch.Tensor, output_path: Optional[Path] = None,\n",
    "        hop_length: int = 256, n_fft: int = 1024, log_scale: bool = True,\n",
    "        phoneme_durations: Optional[List[int]] = None\n",
    "):\n",
    "    if output_path is not None and output_path.is_file():\n",
    "        return\n",
    "\n",
    "    assert len(waveform.shape) == 2 and waveform.shape[0] == 1\n",
    "    waveform = waveform.view(1, 1, waveform.shape[1])\n",
    "    waveform = F.pad(\n",
    "        waveform.unsqueeze(1), [n_fft // 2, n_fft // 2, 0, 0],\n",
    "        mode=\"reflect\"\n",
    "    )\n",
    "    waveform = waveform.squeeze(1)\n",
    "\n",
    "    fourier_basis = np.fft.fft(np.eye(n_fft))\n",
    "    cutoff = int((n_fft / 2 + 1))\n",
    "    fourier_basis = np.vstack(\n",
    "        [np.real(fourier_basis[:cutoff, :]),\n",
    "         np.imag(fourier_basis[:cutoff, :])]\n",
    "    )\n",
    "\n",
    "    forward_basis = torch.FloatTensor(fourier_basis[:, None, :])\n",
    "    forward_transform = F.conv1d(\n",
    "        waveform, forward_basis, stride=hop_length, padding=0\n",
    "    )\n",
    "\n",
    "    real_part = forward_transform[:, :cutoff, :]\n",
    "    imag_part = forward_transform[:, cutoff:, :]\n",
    "    magnitude = torch.sqrt(real_part ** 2 + imag_part ** 2)\n",
    "    energy = torch.norm(magnitude, dim=1).squeeze(0).numpy()\n",
    "\n",
    "    if phoneme_durations is not None:\n",
    "        energy = trim_or_pad_to_target_length(energy, sum(phoneme_durations))\n",
    "        d_cumsum = np.cumsum(np.concatenate([np.array([0]), phoneme_durations]))\n",
    "        energy = np.array(\n",
    "            [\n",
    "                np.mean(energy[d_cumsum[i - 1]: d_cumsum[i]])\n",
    "                for i in range(1, len(d_cumsum))\n",
    "            ]\n",
    "        )\n",
    "        assert len(energy) == len(phoneme_durations)\n",
    "\n",
    "    if log_scale:\n",
    "        energy = np.log(energy + 1)\n",
    "\n",
    "    if output_path is not None:\n",
    "        np.save(output_path.as_posix(), energy)\n",
    "    else:\n",
    "        return energy\n",
    "\n",
    "\n",
    "def get_global_cmvn(feature_root: Path, output_path: Optional[Path] = None):\n",
    "    mean_x, mean_x2, n_frames = None, None, 0\n",
    "    feature_paths = feature_root.glob(\"*.npy\")\n",
    "    for p in tqdm(feature_paths):\n",
    "        with open(p, 'rb') as f:\n",
    "            frames = np.load(f).squeeze()\n",
    "\n",
    "        n_frames += frames.shape[0]\n",
    "\n",
    "        cur_mean_x = frames.sum(axis=0)\n",
    "        if mean_x is None:\n",
    "            mean_x = cur_mean_x\n",
    "        else:\n",
    "            mean_x += cur_mean_x\n",
    "\n",
    "        cur_mean_x2 = (frames ** 2).sum(axis=0)\n",
    "        if mean_x2 is None:\n",
    "            mean_x2 = cur_mean_x2\n",
    "        else:\n",
    "            mean_x2 += cur_mean_x2\n",
    "\n",
    "    mean_x /= n_frames\n",
    "    mean_x2 /= n_frames\n",
    "    var_x = mean_x2 - mean_x ** 2\n",
    "    std_x = np.sqrt(np.maximum(var_x, 1e-10))\n",
    "\n",
    "    if output_path is not None:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            np.savez(f, mean=mean_x, std=std_x)\n",
    "    else:\n",
    "        return {\"mean\": mean_x, \"std\": std_x}\n",
    "\n",
    "\n",
    "def ipa_phonemize(text, lang=\"en-us\", use_g2p=False):\n",
    "    if use_g2p:\n",
    "        assert lang == \"en-us\", \"g2pE phonemizer only works for en-us\"\n",
    "        try:\n",
    "            from g2p_en import G2p\n",
    "            g2p = G2p()\n",
    "            return \" \".join(\"|\" if p == \" \" else p for p in g2p(text))\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install phonemizer: pip install g2p_en\"\n",
    "            )\n",
    "    else:\n",
    "        try:\n",
    "            from phonemizer import phonemize\n",
    "            from phonemizer.separator import Separator\n",
    "            return phonemize(\n",
    "                text, backend='espeak', language=lang,\n",
    "                separator=Separator(word=\"| \", phone=\" \")\n",
    "            )\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install phonemizer: pip install phonemizer\"\n",
    "            )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ForceAlignmentInfo(object):\n",
    "    tokens: List[str]\n",
    "    frame_durations: List[int]\n",
    "    start_sec: Optional[float]\n",
    "    end_sec: Optional[float]\n",
    "\n",
    "\n",
    "def get_mfa_alignment_by_sample_id(\n",
    "        textgrid_zip_path: str, sample_id: str, sample_rate: int,\n",
    "        hop_length: int, silence_phones: List[str] = (\"sil\", \"sp\", \"spn\")\n",
    ") -> ForceAlignmentInfo:\n",
    "    try:\n",
    "        import tgt\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install TextGridTools: pip install tgt\")\n",
    "\n",
    "    filename = f\"{sample_id}.TextGrid\"\n",
    "    out_root = Path(tempfile.gettempdir())\n",
    "    tgt_path = out_root / filename\n",
    "    with zipfile.ZipFile(textgrid_zip_path) as f_zip:\n",
    "        f_zip.extract(filename, path=out_root)\n",
    "    textgrid = tgt.io.read_textgrid(tgt_path.as_posix())\n",
    "    os.remove(tgt_path)\n",
    "\n",
    "    phones, frame_durations = [], []\n",
    "    start_sec, end_sec, end_idx = 0, 0, 0\n",
    "    for t in textgrid.get_tier_by_name(\"phones\")._objects:\n",
    "        s, e, p = t.start_time, t.end_time, t.text\n",
    "        # Trim leading silences\n",
    "        if len(phones) == 0:\n",
    "            if p in silence_phones:\n",
    "                continue\n",
    "            else:\n",
    "                start_sec = s\n",
    "        phones.append(p)\n",
    "        if p not in silence_phones:\n",
    "            end_sec = e\n",
    "            end_idx = len(phones)\n",
    "        r = sample_rate / hop_length\n",
    "        frame_durations.append(int(np.round(e * r) - np.round(s * r)))\n",
    "    # Trim tailing silences\n",
    "    phones = phones[:end_idx]\n",
    "    frame_durations = frame_durations[:end_idx]\n",
    "\n",
    "    return ForceAlignmentInfo(\n",
    "        tokens=phones, frame_durations=frame_durations, start_sec=start_sec,\n",
    "        end_sec=end_sec\n",
    "    )\n",
    "\n",
    "\n",
    "def get_mfa_alignment(\n",
    "        textgrid_zip_path: str, sample_ids: List[str], sample_rate: int,\n",
    "        hop_length: int\n",
    ") -> Dict[str, ForceAlignmentInfo]:\n",
    "    return {\n",
    "        i: get_mfa_alignment_by_sample_id(\n",
    "            textgrid_zip_path, i, sample_rate, hop_length\n",
    "        ) for i in tqdm(sample_ids)\n",
    "    }\n",
    "\n",
    "\n",
    "def get_unit_alignment(\n",
    "        id_to_unit_tsv_path: str, sample_ids: List[str]\n",
    ") -> Dict[str, ForceAlignmentInfo]:\n",
    "    id_to_units = {\n",
    "        e[\"id\"]: e[\"units\"] for e in load_tsv_to_dicts(id_to_unit_tsv_path)\n",
    "    }\n",
    "    id_to_units = {i: id_to_units[i].split() for i in sample_ids}\n",
    "    id_to_units_collapsed = {\n",
    "        i: [uu for uu, _ in groupby(u)] for i, u in id_to_units.items()\n",
    "    }\n",
    "    id_to_durations = {\n",
    "        i: [len(list(g)) for _, g in groupby(u)] for i, u in id_to_units.items()\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        i: ForceAlignmentInfo(\n",
    "            tokens=id_to_units_collapsed[i], frame_durations=id_to_durations[i],\n",
    "            start_sec=None, end_sec=None\n",
    "        )\n",
    "        for i in sample_ids\n",
    "    }\n",
    "\n",
    "\n",
    "def get_feature_value_min_max(feature_paths: List[str]):\n",
    "    v_min, v_max = 1e-8, -1e-8\n",
    "    for p in tqdm(feature_paths):\n",
    "        _path, slice_ptr = parse_path(p)\n",
    "        assert len(slice_ptr) == 2\n",
    "        byte_data = read_from_stored_zip(_path, slice_ptr[0], slice_ptr[1])\n",
    "        assert is_npy_data(byte_data)\n",
    "        path_or_fp = io.BytesIO(byte_data)\n",
    "        features = np.load(path_or_fp).squeeze()\n",
    "        v_min = min(v_min, features.min().item())\n",
    "        v_max = max(v_max, features.max().item())\n",
    "    return v_min, v_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fb175-ce53-45fa-b1a3-098d9a327e93",
   "metadata": {},
   "source": [
    "# LOG-MEL SPECTROGRAM FEATURE MANIFEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3be6086-4199-4b3d-8840-dd98a021e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFeatureManifest(args):\n",
    "    assert \"train\" in args.splits\n",
    "    out_root = Path(args.output_root).absolute()\n",
    "    out_root.mkdir(exist_ok=True)\n",
    "\n",
    "    print(\"Fetching data...\")\n",
    "    audio_manifest_root = Path(args.audio_manifest_root).absolute()\n",
    "    samples = []\n",
    "    for s in args.splits:\n",
    "        for e in load_tsv_to_dicts(audio_manifest_root / f\"{s}.audio.tsv\"):\n",
    "            e[\"split\"] = s\n",
    "            samples.append(e)\n",
    "    sample_ids = [s[\"id\"] for s in samples]\n",
    "\n",
    "    # Get alignment info\n",
    "    id_to_alignment = None\n",
    "    if args.textgrid_zip is not None:\n",
    "        assert args.id_to_units_tsv is None\n",
    "        id_to_alignment = get_mfa_alignment(\n",
    "            args.textgrid_zip, sample_ids, args.sample_rate, args.hop_length\n",
    "        )\n",
    "    elif args.id_to_units_tsv is not None:\n",
    "        # assume identical hop length on the unit sequence\n",
    "        id_to_alignment = get_unit_alignment(args.id_to_units_tsv, sample_ids)\n",
    "\n",
    "    # Extract features and pack features into ZIP\n",
    "    feature_name = \"logmelspec80\"\n",
    "    zip_path = out_root / f\"{feature_name}.zip\"\n",
    "    pitch_zip_path = out_root / \"pitch.zip\"\n",
    "    energy_zip_path = out_root / \"energy.zip\"\n",
    "    gcmvn_npz_path = out_root / \"gcmvn_stats.npz\"\n",
    "    if zip_path.exists() and gcmvn_npz_path.exists():\n",
    "        print(f\"{zip_path} and {gcmvn_npz_path} exist.\")\n",
    "    else:\n",
    "        feature_root = out_root / feature_name\n",
    "        feature_root.mkdir(exist_ok=True)\n",
    "        pitch_root = out_root / \"pitch\"\n",
    "        energy_root = out_root / \"energy\"\n",
    "        if args.add_fastspeech_targets:\n",
    "            pitch_root.mkdir(exist_ok=True)\n",
    "            energy_root.mkdir(exist_ok=True)\n",
    "        print(\"Extracting Mel spectrogram features...\")\n",
    "        for sample in tqdm(samples):\n",
    "            waveform, sample_rate = torchaudio.load(sample[\"audio\"])\n",
    "            waveform, sample_rate = convert_waveform(\n",
    "                waveform, sample_rate, normalize_volume=args.normalize_volume,\n",
    "                to_sample_rate=args.sample_rate\n",
    "            )\n",
    "            sample_id = sample[\"id\"]\n",
    "            target_length = None\n",
    "            if id_to_alignment is not None:\n",
    "                a = id_to_alignment[sample_id]\n",
    "                target_length = sum(a.frame_durations)\n",
    "                if a.start_sec is not None and a.end_sec is not None:\n",
    "                    start_frame = int(a.start_sec * sample_rate)\n",
    "                    end_frame = int(a.end_sec * sample_rate)\n",
    "                    waveform = waveform[:, start_frame: end_frame]\n",
    "            extract_logmel_spectrogram(\n",
    "                waveform, sample_rate, feature_root / f\"{sample_id}.npy\",\n",
    "                win_length=args.win_length, hop_length=args.hop_length,\n",
    "                n_fft=args.n_fft, n_mels=args.n_mels, f_min=args.f_min,\n",
    "                f_max=args.f_max, target_length=target_length\n",
    "            )\n",
    "            if args.add_fastspeech_targets:\n",
    "                assert id_to_alignment is not None\n",
    "                extract_pitch(\n",
    "                    waveform, sample_rate, pitch_root / f\"{sample_id}.npy\",\n",
    "                    hop_length=args.hop_length, log_scale=True,\n",
    "                    phoneme_durations=id_to_alignment[sample_id].frame_durations\n",
    "                )\n",
    "                extract_energy(\n",
    "                    waveform, energy_root / f\"{sample_id}.npy\",\n",
    "                    hop_length=args.hop_length, n_fft=args.n_fft,\n",
    "                    log_scale=True,\n",
    "                    phoneme_durations=id_to_alignment[sample_id].frame_durations\n",
    "                )\n",
    "        print(\"ZIPing features...\")\n",
    "        create_zip(feature_root, zip_path)\n",
    "        get_global_cmvn(feature_root, gcmvn_npz_path)\n",
    "        shutil.rmtree(feature_root)\n",
    "        if args.add_fastspeech_targets:\n",
    "            create_zip(pitch_root, pitch_zip_path)\n",
    "            shutil.rmtree(pitch_root)\n",
    "            create_zip(energy_root, energy_zip_path)\n",
    "            shutil.rmtree(energy_root)\n",
    "\n",
    "    print(\"Fetching ZIP manifest...\")\n",
    "    audio_paths, audio_lengths = get_zip_manifest(zip_path)\n",
    "    pitch_paths, pitch_lengths, energy_paths, energy_lengths = [None] * 4\n",
    "    if args.add_fastspeech_targets:\n",
    "        pitch_paths, pitch_lengths = get_zip_manifest(pitch_zip_path)\n",
    "        energy_paths, energy_lengths = get_zip_manifest(energy_zip_path)\n",
    "    # Generate TSV manifest\n",
    "    print(\"Generating manifest...\")\n",
    "    id_to_cer = None\n",
    "    if args.cer_threshold is not None:\n",
    "        assert Path(args.cer_tsv_path).is_file()\n",
    "        id_to_cer = {\n",
    "            x[\"id\"]: x[\"uer\"] for x in load_tsv_to_dicts(args.cer_tsv_path)\n",
    "        }\n",
    "    manifest_by_split = {split: defaultdict(list) for split in args.splits}\n",
    "    for sample in tqdm(samples):\n",
    "        sample_id, split = sample[\"id\"], sample[\"split\"]\n",
    "\n",
    "        if args.snr_threshold is not None and \"snr\" in sample \\\n",
    "                and sample[\"snr\"] < args.snr_threshold:\n",
    "            continue\n",
    "        if args.cer_threshold is not None \\\n",
    "                and id_to_cer[sample_id] > args.cer_threhold:\n",
    "            continue\n",
    "\n",
    "        normalized_utt = sample[\"tgt_text\"]\n",
    "        if id_to_alignment is not None:\n",
    "            normalized_utt = \" \".join(id_to_alignment[sample_id].tokens)\n",
    "        elif args.ipa_vocab:\n",
    "            normalized_utt = ipa_phonemize(\n",
    "                normalized_utt, lang=args.lang, use_g2p=args.use_g2p\n",
    "            )\n",
    "        manifest_by_split[split][\"id\"].append(sample_id)\n",
    "        manifest_by_split[split][\"audio\"].append(audio_paths[sample_id])\n",
    "        manifest_by_split[split][\"n_frames\"].append(audio_lengths[sample_id])\n",
    "        manifest_by_split[split][\"tgt_text\"].append(normalized_utt)\n",
    "        manifest_by_split[split][\"speaker\"].append(sample[\"speaker\"])\n",
    "        manifest_by_split[split][\"src_text\"].append(sample[\"src_text\"])\n",
    "        if args.add_fastspeech_targets:\n",
    "            assert id_to_alignment is not None\n",
    "            duration = \" \".join(\n",
    "                str(d) for d in id_to_alignment[sample_id].frame_durations\n",
    "            )\n",
    "            manifest_by_split[split][\"duration\"].append(duration)\n",
    "            manifest_by_split[split][\"pitch\"].append(pitch_paths[sample_id])\n",
    "            manifest_by_split[split][\"energy\"].append(energy_paths[sample_id])\n",
    "    for split in args.splits:\n",
    "        save_df_to_tsv(\n",
    "            pd.DataFrame.from_dict(manifest_by_split[split]),\n",
    "            out_root / f\"{split}.tsv\"\n",
    "        )\n",
    "    # Generate vocab\n",
    "    vocab_name, spm_filename = None, None\n",
    "    if id_to_alignment is not None or args.ipa_vocab:\n",
    "        vocab = Counter()\n",
    "        for t in manifest_by_split[\"train\"][\"tgt_text\"]:\n",
    "            vocab.update(t.split(\" \"))\n",
    "        vocab_name = \"vocab.txt\"\n",
    "        with open(out_root / vocab_name, \"w\") as f:\n",
    "            for s, c in vocab.most_common():\n",
    "                f.write(f\"{s} {c}\\n\")\n",
    "    else:\n",
    "        spm_filename_prefix = \"spm_char\"\n",
    "        spm_filename = f\"{spm_filename_prefix}.model\"\n",
    "        with NamedTemporaryFile(mode=\"w\") as f:\n",
    "            for t in manifest_by_split[\"train\"][\"tgt_text\"]:\n",
    "                f.write(t + \"\\n\")\n",
    "            f.flush()  # needed to ensure gen_vocab sees dumped text\n",
    "            gen_vocab(Path(f.name), out_root / spm_filename_prefix, \"char\")\n",
    "    # Generate speaker list\n",
    "    speakers = sorted({sample[\"speaker\"] for sample in samples})\n",
    "    speakers_path = out_root / \"speakers.txt\"\n",
    "    with open(speakers_path, \"w\") as f:\n",
    "        for speaker in speakers:\n",
    "            f.write(f\"{speaker}\\n\")\n",
    "    # Generate config YAML\n",
    "    win_len_t = args.win_length / args.sample_rate\n",
    "    hop_len_t = args.hop_length / args.sample_rate\n",
    "    extra = {\n",
    "        \"sample_rate\": args.sample_rate,\n",
    "        \"features\": {\n",
    "            \"type\": \"spectrogram+melscale+log\",\n",
    "            \"eps\": 1e-5, \"n_mels\": args.n_mels, \"n_fft\": args.n_fft,\n",
    "            \"window_fn\": \"hann\", \"win_length\": args.win_length,\n",
    "            \"hop_length\": args.hop_length, \"sample_rate\": args.sample_rate,\n",
    "            \"win_len_t\": win_len_t, \"hop_len_t\": hop_len_t,\n",
    "            \"f_min\": args.f_min, \"f_max\": args.f_max,\n",
    "            \"n_stft\": args.n_fft // 2 + 1\n",
    "        }\n",
    "    }\n",
    "    if len(speakers) > 1:\n",
    "        extra[\"speaker_set_filename\"] = \"speakers.txt\"\n",
    "    if args.add_fastspeech_targets:\n",
    "        pitch_min, pitch_max = get_feature_value_min_max(\n",
    "            [(out_root / n).as_posix() for n in pitch_paths.values()]\n",
    "        )\n",
    "        energy_min, energy_max = get_feature_value_min_max(\n",
    "            [(out_root / n).as_posix() for n in energy_paths.values()]\n",
    "        )\n",
    "        extra[\"features\"][\"pitch_min\"] = pitch_min\n",
    "        extra[\"features\"][\"pitch_max\"] = pitch_max\n",
    "        extra[\"features\"][\"energy_min\"] = energy_min\n",
    "        extra[\"features\"][\"energy_max\"] = energy_max\n",
    "    gen_config_yaml(\n",
    "        out_root, spm_filename=spm_filename, vocab_name=vocab_name,\n",
    "        audio_root=out_root.as_posix(), input_channels=None,\n",
    "        input_feat_per_channel=None, specaugment_policy=None,\n",
    "        cmvn_type=\"global\", gcmvn_path=gcmvn_npz_path, extra=extra\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaac8d79-2b16-4c47-aad7-68caab5c8e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(audio_manifest_root='/workspace/process/LJ/', output_root='/workspace/process/LJ/', splits=['train', 'dev', 'test'], ipa_vocab=True, use_g2p=True, lang='en-us', win_length=1024, hop_length=256, n_fft=1024, n_mels=80, f_min=20, f_max=8000, sample_rate=22050, normalize_volume=False, textgrid_zip=None, id_to_units_tsv=None, add_fastspeech_targets=False, snr_threshold=None, cer_threshold=None, cer_tsv_path='')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--audio-manifest-root\", \"-m\", required=True, type=str)\n",
    "parser.add_argument(\"--output-root\", \"-o\", required=True, type=str)\n",
    "parser.add_argument(\"--splits\", \"-s\", type=str, nargs=\"+\",\n",
    "                    default=[\"train\", \"dev\", \"test\"])\n",
    "parser.add_argument(\"--ipa-vocab\", action=\"store_true\")\n",
    "parser.add_argument(\"--use-g2p\", action=\"store_true\")\n",
    "parser.add_argument(\"--lang\", type=str, default=\"en-us\")\n",
    "parser.add_argument(\"--win-length\", type=int, default=1024)\n",
    "parser.add_argument(\"--hop-length\", type=int, default=256)\n",
    "parser.add_argument(\"--n-fft\", type=int, default=1024)\n",
    "parser.add_argument(\"--n-mels\", type=int, default=80)\n",
    "parser.add_argument(\"--f-min\", type=int, default=20)\n",
    "parser.add_argument(\"--f-max\", type=int, default=8000)\n",
    "parser.add_argument(\"--sample-rate\", type=int, default=22050)\n",
    "parser.add_argument(\"--normalize-volume\", \"-n\", action=\"store_true\")\n",
    "parser.add_argument(\"--textgrid-zip\", type=str, default=None)\n",
    "parser.add_argument(\"--id-to-units-tsv\", type=str, default=None)\n",
    "parser.add_argument(\"--add-fastspeech-targets\", action=\"store_true\")\n",
    "parser.add_argument(\"--snr-threshold\", type=float, default=None)\n",
    "parser.add_argument(\"--cer-threshold\", type=float, default=None)\n",
    "parser.add_argument(\"--cer-tsv-path\", type=str, default=\"\")\n",
    "args = parser.parse_args(['--audio-manifest-root', out_path, '--output-root', out_path, '--ipa-vocab', '--use-g2p']) #, '--add-fastspeech-targets'])\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a17983-0d23-4d8f-8793-da7cfcfbf3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...\n",
      "/workspace/process/LJ/logmelspec80.zip and /workspace/process/LJ/gcmvn_stats.npz exist.\n",
      "Fetching ZIP manifest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13100/13100 [00:23<00:00, 552.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating manifest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                             | 30/13100 [00:21<2:30:29,  1.45it/s]"
     ]
    }
   ],
   "source": [
    "processFeatureManifest(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7644c9d8-0076-416e-9c0d-110936f75815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ac31c81-e930-42cf-9d00-19f927235a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA TITAN RTX'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e279d26-a2d3-4225-a85e-7e1796467670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
